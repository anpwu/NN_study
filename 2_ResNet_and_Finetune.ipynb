{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **卷积神经网络进阶——Lenet5和ResNet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LeNet5 [1] 诞生于 1994 年，是最早的卷积神经网络之一，并且推动了深度学习领域的发展。\n",
    "\n",
    "核心思想:图像的特征分布在整张图像上，带有可学习参数的卷积可以用少量的参数在多个位置上有效地提取相似特征。和将所有像素作为一个大型多层神经网络的单独输入不同, LeNet5认为图像数据具有很强的空间相关性，如果使用图像中独立的像素作为输入特征则无法利用时空相关性。\n",
    "\n",
    "[1] LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Lenet5网络结构展示\n",
    "\n",
    "<img src=\"./picture/lenet5_1.png\" width=800>\n",
    "\n",
    "LeNet-5是一个较简单的卷积神经网络。下图显示了其结构：输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层。下面我们主要介绍卷积层和池化层。\n",
    "\n",
    "输入的二维图像，先经过两次卷积层到池化层，再经过全连接层，最后使用softmax分类作为输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LeNet(\n",
       "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "lenet = LeNet()\n",
    "lenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 准备MNIST数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Hyper Parameters\n",
    "EPOCH = 1           \n",
    "BATCH_SIZE = 50\n",
    "LR = 0.001 \n",
    "DOWNLOAD_MNIST = True\n",
    "\n",
    "# Mnist 手写数字\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/', \n",
    "    train=True,  # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST,\n",
    ")\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='./data/', train=False)\n",
    "\n",
    "# 批训练 50samples, 1 channel, 28x28 (50, 1, 28, 28)\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# 测试数据集\n",
    "test_x = torch.unsqueeze(test_data.data, dim=1).type(torch.FloatTensor)/255. \n",
    "test_y = test_data.targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 选择优化方式和定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lenet.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\ 1200:  tensor(2.3146, grad_fn=<NllLossBackward>)\n",
      "200 \\ 1200:  tensor(0.7246, grad_fn=<NllLossBackward>)\n",
      "400 \\ 1200:  tensor(0.4504, grad_fn=<NllLossBackward>)\n",
      "600 \\ 1200:  tensor(0.4945, grad_fn=<NllLossBackward>)\n",
      "800 \\ 1200:  tensor(0.2684, grad_fn=<NllLossBackward>)\n",
      "1000 \\ 1200:  tensor(0.3877, grad_fn=<NllLossBackward>)\n",
      "1200 \\ 1200:  tensor(0.1223, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = lenet(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 200 == 0:\n",
    "            print(step, \"\\\\ 1200: \", loss)\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(1200, \"\\\\ 1200: \", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 2 1 0 4 1 4 9 5 9] prediction number\n",
      "[7 2 1 0 4 1 4 9 5 9] real number\n",
      "The accuracy:  0.9148\n"
     ]
    }
   ],
   "source": [
    "test_output = lenet(test_x)\n",
    "pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "print(pred_y[:10], 'prediction number')\n",
    "print(test_y[:10].numpy(), 'real number')\n",
    "\n",
    "accuracy = 0\n",
    "for i in range(len(test_y)):\n",
    "    if pred_y[i] == test_y[i]:\n",
    "        accuracy += 1\n",
    "accuracy /= len(test_y)\n",
    "print(\"The accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 模型调参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mnist 手写数字\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./data/', \n",
    "    train=True,  # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),   \n",
    "    download=False,\n",
    ")\n",
    "\n",
    "# 测试数据集\n",
    "test_data = torchvision.datasets.MNIST(root='./data/', train=False)\n",
    "test_x = torch.unsqueeze(test_data.data, dim=1).type(torch.FloatTensor)/255. \n",
    "test_y = test_data.targets\n",
    "\n",
    "def get_accuracy(pred_y, test_y):\n",
    "    accuracy = 0\n",
    "    for i in range(len(test_y)):\n",
    "        if pred_y[i] == test_y[i]:\n",
    "            accuracy += 1\n",
    "    accuracy /= len(test_y)\n",
    "    return accuracy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 模型整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\ 469: 2.3161559104919434\n",
      "100 \\ 469: 0.8792778849601746\n",
      "200 \\ 469: 0.5495938658714294\n",
      "300 \\ 469: 0.6191794276237488\n",
      "400 \\ 469: 0.32914307713508606\n",
      "468 \\ 469: 0.6522848010063171\n",
      "The accuracy:  0.8816\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 学习率的影响(lr = 0.5, 0.1, 0.01, 0.001, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.5\n",
      "0 \\ 469: 2.294623374938965\n",
      "100 \\ 469: 2.305206060409546\n",
      "200 \\ 469: 2.293544292449951\n",
      "300 \\ 469: 2.3251876831054688\n",
      "400 \\ 469: 2.2903807163238525\n",
      "468 \\ 469: 2.3408987522125244\n",
      "The accuracy:  0.1032\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}\".format(LR))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.1\n",
      "0 \\ 469: 2.3095030784606934\n",
      "100 \\ 469: 2.318173885345459\n",
      "200 \\ 469: 2.3053905963897705\n",
      "300 \\ 469: 2.293431282043457\n",
      "400 \\ 469: 2.3145930767059326\n",
      "468 \\ 469: 2.293591260910034\n",
      "The accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.1\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}\".format(LR))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.01\n",
      "0 \\ 469: 2.3097434043884277\n",
      "100 \\ 469: 0.5020248889923096\n",
      "200 \\ 469: 0.4101569950580597\n",
      "300 \\ 469: 0.43195873498916626\n",
      "400 \\ 469: 0.33610889315605164\n",
      "468 \\ 469: 0.39019644260406494\n",
      "The accuracy:  0.9141\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}\".format(LR))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.001\n",
      "0 \\ 469: 2.3091623783111572\n",
      "100 \\ 469: 0.8119509816169739\n",
      "200 \\ 469: 0.5555205941200256\n",
      "300 \\ 469: 0.49830445647239685\n",
      "400 \\ 469: 0.27350348234176636\n",
      "468 \\ 469: 0.44896045327186584\n",
      "The accuracy:  0.9019\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}\".format(LR))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \\ 469: 2.3173182010650635\n",
      "100 \\ 469: 2.2582955360412598\n",
      "200 \\ 469: 2.012465715408325\n",
      "300 \\ 469: 1.549019455909729\n",
      "400 \\ 469: 1.2246626615524292\n",
      "468 \\ 469: 1.1059397459030151\n",
      "The accuracy:  0.6777\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1     \n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"{} \\\\ {}: {}\".format(step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 5 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 算法收敛(Epoch) (LR=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0 \\ 469: 2.31659197807312\n",
      "epoch 0, batch 100 \\ 469: 2.2423763275146484\n",
      "epoch 0, batch 200 \\ 469: 1.9429116249084473\n",
      "epoch 0, batch 300 \\ 469: 1.4636590480804443\n",
      "epoch 0, batch 400 \\ 469: 1.0838598012924194\n",
      "epoch 0, batch 468 \\ 469: 0.9720259308815002\n",
      "The accuracy:  0.6928\n",
      "epoch 1, batch 0 \\ 469: 0.8370915651321411\n",
      "epoch 1, batch 100 \\ 469: 0.9007120728492737\n",
      "epoch 1, batch 200 \\ 469: 0.8205913305282593\n",
      "epoch 1, batch 300 \\ 469: 0.8055069446563721\n",
      "epoch 1, batch 400 \\ 469: 0.6228863596916199\n",
      "epoch 1, batch 468 \\ 469: 0.8032221794128418\n",
      "The accuracy:  0.7974\n",
      "epoch 2, batch 0 \\ 469: 0.752005398273468\n",
      "epoch 2, batch 100 \\ 469: 0.5976746082305908\n",
      "epoch 2, batch 200 \\ 469: 0.7879090309143066\n",
      "epoch 2, batch 300 \\ 469: 0.5923303365707397\n",
      "epoch 2, batch 400 \\ 469: 0.5047240257263184\n",
      "epoch 2, batch 468 \\ 469: 0.4320560395717621\n",
      "The accuracy:  0.835\n",
      "epoch 3, batch 0 \\ 469: 0.6913126111030579\n",
      "epoch 3, batch 100 \\ 469: 0.609498143196106\n",
      "epoch 3, batch 200 \\ 469: 0.4541616141796112\n",
      "epoch 3, batch 300 \\ 469: 0.4515027701854706\n",
      "epoch 3, batch 400 \\ 469: 0.35803860425949097\n",
      "epoch 3, batch 468 \\ 469: 0.3968457877635956\n",
      "The accuracy:  0.86\n",
      "epoch 4, batch 0 \\ 469: 0.5471302270889282\n",
      "epoch 4, batch 100 \\ 469: 0.40932759642601013\n",
      "epoch 4, batch 200 \\ 469: 0.44937244057655334\n",
      "epoch 4, batch 300 \\ 469: 0.4451426863670349\n",
      "epoch 4, batch 400 \\ 469: 0.6844183802604675\n",
      "epoch 4, batch 468 \\ 469: 0.5024667382240295\n",
      "The accuracy:  0.8738\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 5     \n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, batch 0 \\ 469: 0.5440018177032471\n",
      "epoch 5, batch 100 \\ 469: 0.3162383735179901\n",
      "epoch 5, batch 200 \\ 469: 0.3551766574382782\n",
      "epoch 5, batch 300 \\ 469: 0.41187551617622375\n",
      "epoch 5, batch 400 \\ 469: 0.37308067083358765\n",
      "epoch 5, batch 468 \\ 469: 0.3674497902393341\n",
      "The accuracy:  0.8833\n",
      "epoch 6, batch 0 \\ 469: 0.38901907205581665\n",
      "epoch 6, batch 100 \\ 469: 0.3291124999523163\n",
      "epoch 6, batch 200 \\ 469: 0.33019712567329407\n",
      "epoch 6, batch 300 \\ 469: 0.5645003914833069\n",
      "epoch 6, batch 400 \\ 469: 0.3658604919910431\n",
      "epoch 6, batch 468 \\ 469: 0.2446144074201584\n",
      "The accuracy:  0.8985\n",
      "epoch 7, batch 0 \\ 469: 0.3567560613155365\n",
      "epoch 7, batch 100 \\ 469: 0.44791051745414734\n",
      "epoch 7, batch 200 \\ 469: 0.27363121509552\n",
      "epoch 7, batch 300 \\ 469: 0.3486737906932831\n",
      "epoch 7, batch 400 \\ 469: 0.36492839455604553\n",
      "epoch 7, batch 468 \\ 469: 0.31886303424835205\n",
      "The accuracy:  0.8946\n",
      "epoch 8, batch 0 \\ 469: 0.2899123728275299\n",
      "epoch 8, batch 100 \\ 469: 0.3328332304954529\n",
      "epoch 8, batch 200 \\ 469: 0.4377455711364746\n",
      "epoch 8, batch 300 \\ 469: 0.3110761344432831\n",
      "epoch 8, batch 400 \\ 469: 0.28994032740592957\n",
      "epoch 8, batch 468 \\ 469: 0.4404700696468353\n",
      "The accuracy:  0.9046\n",
      "epoch 9, batch 0 \\ 469: 0.2326686829328537\n",
      "epoch 9, batch 100 \\ 469: 0.269172728061676\n",
      "epoch 9, batch 200 \\ 469: 0.30238091945648193\n",
      "epoch 9, batch 300 \\ 469: 0.20023474097251892\n",
      "epoch 9, batch 400 \\ 469: 0.2232079654932022\n",
      "epoch 9, batch 468 \\ 469: 0.3675123155117035\n",
      "The accuracy:  0.9088\n"
     ]
    }
   ],
   "source": [
    "# training and testing\n",
    "for epoch in range(5, 5+EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, batch 0 \\ 469: 0.2332538366317749\n",
      "epoch 10, batch 100 \\ 469: 0.31788504123687744\n",
      "epoch 10, batch 200 \\ 469: 0.2959536612033844\n",
      "epoch 10, batch 300 \\ 469: 0.3131331503391266\n",
      "epoch 10, batch 400 \\ 469: 0.34628593921661377\n",
      "epoch 10, batch 468 \\ 469: 0.4246891438961029\n",
      "The accuracy:  0.912\n",
      "epoch 11, batch 0 \\ 469: 0.3116169273853302\n",
      "epoch 11, batch 100 \\ 469: 0.2837075889110565\n",
      "epoch 11, batch 200 \\ 469: 0.20258238911628723\n",
      "epoch 11, batch 300 \\ 469: 0.3190847337245941\n",
      "epoch 11, batch 400 \\ 469: 0.20166929066181183\n",
      "epoch 11, batch 468 \\ 469: 0.1830863207578659\n",
      "The accuracy:  0.9105\n",
      "epoch 12, batch 0 \\ 469: 0.39048275351524353\n",
      "epoch 12, batch 100 \\ 469: 0.47522982954978943\n",
      "epoch 12, batch 200 \\ 469: 0.25806066393852234\n",
      "epoch 12, batch 300 \\ 469: 0.31582459807395935\n",
      "epoch 12, batch 400 \\ 469: 0.4334928095340729\n",
      "epoch 12, batch 468 \\ 469: 0.28257814049720764\n",
      "The accuracy:  0.9197\n",
      "epoch 13, batch 0 \\ 469: 0.2330884486436844\n",
      "epoch 13, batch 100 \\ 469: 0.2094266265630722\n",
      "epoch 13, batch 200 \\ 469: 0.3389566242694855\n",
      "epoch 13, batch 300 \\ 469: 0.24188408255577087\n",
      "epoch 13, batch 400 \\ 469: 0.260231077671051\n",
      "epoch 13, batch 468 \\ 469: 0.3575797975063324\n",
      "The accuracy:  0.9187\n",
      "epoch 14, batch 0 \\ 469: 0.21313472092151642\n",
      "epoch 14, batch 100 \\ 469: 0.27856218814849854\n",
      "epoch 14, batch 200 \\ 469: 0.3577693700790405\n",
      "epoch 14, batch 300 \\ 469: 0.37594521045684814\n",
      "epoch 14, batch 400 \\ 469: 0.2883667051792145\n",
      "epoch 14, batch 468 \\ 469: 0.28294968605041504\n",
      "The accuracy:  0.921\n"
     ]
    }
   ],
   "source": [
    "# training and testing\n",
    "for epoch in range(10, 10+EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 测试不同学习率的收敛情况 (LR=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, batch 0 \\ 469: 2.3020200729370117\n",
      "epoch 0, batch 100 \\ 469: 0.5564085841178894\n",
      "epoch 0, batch 200 \\ 469: 0.4021049737930298\n",
      "epoch 0, batch 300 \\ 469: 0.39024990797042847\n",
      "epoch 0, batch 400 \\ 469: 0.4447093904018402\n",
      "epoch 0, batch 468 \\ 469: 0.33565524220466614\n",
      "The accuracy:  0.9085\n",
      "epoch 1, batch 0 \\ 469: 0.25280675292015076\n",
      "epoch 1, batch 100 \\ 469: 0.3594478368759155\n",
      "epoch 1, batch 200 \\ 469: 0.3745402693748474\n",
      "epoch 1, batch 300 \\ 469: 0.20888279378414154\n",
      "epoch 1, batch 400 \\ 469: 0.19976621866226196\n",
      "epoch 1, batch 468 \\ 469: 0.3292265236377716\n",
      "The accuracy:  0.9251\n",
      "epoch 2, batch 0 \\ 469: 0.17992641031742096\n",
      "epoch 2, batch 100 \\ 469: 0.2013881802558899\n",
      "epoch 2, batch 200 \\ 469: 0.22156307101249695\n",
      "epoch 2, batch 300 \\ 469: 0.19721288979053497\n",
      "epoch 2, batch 400 \\ 469: 0.13821081817150116\n",
      "epoch 2, batch 468 \\ 469: 0.23379887640476227\n",
      "The accuracy:  0.9294\n",
      "epoch 3, batch 0 \\ 469: 0.22159135341644287\n",
      "epoch 3, batch 100 \\ 469: 0.12662054598331451\n",
      "epoch 3, batch 200 \\ 469: 0.18097424507141113\n",
      "epoch 3, batch 300 \\ 469: 0.4048800766468048\n",
      "epoch 3, batch 400 \\ 469: 0.12645459175109863\n",
      "epoch 3, batch 468 \\ 469: 0.20705397427082062\n",
      "The accuracy:  0.9288\n",
      "epoch 4, batch 0 \\ 469: 0.4036789536476135\n",
      "epoch 4, batch 100 \\ 469: 0.5623806715011597\n",
      "epoch 4, batch 200 \\ 469: 0.1704784631729126\n",
      "epoch 4, batch 300 \\ 469: 0.15701092779636383\n",
      "epoch 4, batch 400 \\ 469: 0.19741122424602509\n",
      "epoch 4, batch 468 \\ 469: 0.2113504856824875\n",
      "The accuracy:  0.9364\n",
      "epoch 5, batch 0 \\ 469: 0.21073468029499054\n",
      "epoch 5, batch 100 \\ 469: 0.20359796285629272\n",
      "epoch 5, batch 200 \\ 469: 0.11142163723707199\n",
      "epoch 5, batch 300 \\ 469: 0.28076526522636414\n",
      "epoch 5, batch 400 \\ 469: 0.13186559081077576\n",
      "epoch 5, batch 468 \\ 469: 0.2926952540874481\n",
      "The accuracy:  0.9221\n",
      "epoch 6, batch 0 \\ 469: 0.1601833999156952\n",
      "epoch 6, batch 100 \\ 469: 0.35543379187583923\n",
      "epoch 6, batch 200 \\ 469: 0.33412548899650574\n",
      "epoch 6, batch 300 \\ 469: 0.3016977310180664\n",
      "epoch 6, batch 400 \\ 469: 0.20350602269172668\n",
      "epoch 6, batch 468 \\ 469: 0.1354721039533615\n",
      "The accuracy:  0.9272\n",
      "epoch 7, batch 0 \\ 469: 0.36844295263290405\n",
      "epoch 7, batch 100 \\ 469: 0.27811726927757263\n",
      "epoch 7, batch 200 \\ 469: 0.2614528238773346\n",
      "epoch 7, batch 300 \\ 469: 0.3103159964084625\n",
      "epoch 7, batch 400 \\ 469: 0.18694984912872314\n",
      "epoch 7, batch 468 \\ 469: 0.1946873813867569\n",
      "The accuracy:  0.9385\n",
      "epoch 8, batch 0 \\ 469: 0.06171237677335739\n",
      "epoch 8, batch 100 \\ 469: 0.31521937251091003\n",
      "epoch 8, batch 200 \\ 469: 0.25706276297569275\n",
      "epoch 8, batch 300 \\ 469: 0.0983179435133934\n",
      "epoch 8, batch 400 \\ 469: 0.24569113552570343\n",
      "epoch 8, batch 468 \\ 469: 0.1101059839129448\n",
      "The accuracy:  0.9303\n",
      "epoch 9, batch 0 \\ 469: 0.1756666749715805\n",
      "epoch 9, batch 100 \\ 469: 0.23825858533382416\n",
      "epoch 9, batch 200 \\ 469: 0.24700230360031128\n",
      "epoch 9, batch 300 \\ 469: 0.332686185836792\n",
      "epoch 9, batch 400 \\ 469: 0.19516566395759583\n",
      "epoch 9, batch 468 \\ 469: 0.30110639333724976\n",
      "The accuracy:  0.9327\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10     \n",
    "LR = 0.01\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Batch Size的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.01, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.3033225536346436\n",
      "epoch 0, batch 100 \\ 1200: 0.4699312448501587\n",
      "epoch 0, batch 200 \\ 1200: 0.3083084523677826\n",
      "epoch 0, batch 300 \\ 1200: 0.6657001972198486\n",
      "epoch 0, batch 400 \\ 1200: 0.7186720967292786\n",
      "epoch 0, batch 500 \\ 1200: 0.4065611660480499\n",
      "epoch 0, batch 600 \\ 1200: 0.46238765120506287\n",
      "epoch 0, batch 700 \\ 1200: 0.21865147352218628\n",
      "epoch 0, batch 800 \\ 1200: 0.258233904838562\n",
      "epoch 0, batch 900 \\ 1200: 0.2459692358970642\n",
      "epoch 0, batch 1000 \\ 1200: 0.3176041543483734\n",
      "epoch 0, batch 1100 \\ 1200: 0.20567651093006134\n",
      "epoch 0, batch 1199 \\ 1200: 0.2819063663482666\n",
      "The accuracy:  0.8879\n",
      "epoch 1, batch 0 \\ 1200: 0.4628733694553375\n",
      "epoch 1, batch 100 \\ 1200: 0.29479798674583435\n",
      "epoch 1, batch 200 \\ 1200: 0.78133225440979\n",
      "epoch 1, batch 300 \\ 1200: 0.43220385909080505\n",
      "epoch 1, batch 400 \\ 1200: 0.7156745195388794\n",
      "epoch 1, batch 500 \\ 1200: 0.3404513895511627\n",
      "epoch 1, batch 600 \\ 1200: 0.3325895071029663\n",
      "epoch 1, batch 700 \\ 1200: 0.3814425766468048\n",
      "epoch 1, batch 800 \\ 1200: 0.11151228100061417\n",
      "epoch 1, batch 900 \\ 1200: 0.3679177165031433\n",
      "epoch 1, batch 1000 \\ 1200: 0.4346161186695099\n",
      "epoch 1, batch 1100 \\ 1200: 0.22741959989070892\n",
      "epoch 1, batch 1199 \\ 1200: 0.2741389572620392\n",
      "The accuracy:  0.8849\n",
      "epoch 2, batch 0 \\ 1200: 0.8507550954818726\n",
      "epoch 2, batch 100 \\ 1200: 0.22859390079975128\n",
      "epoch 2, batch 200 \\ 1200: 0.8471064567565918\n",
      "epoch 2, batch 300 \\ 1200: 0.4318956434726715\n",
      "epoch 2, batch 400 \\ 1200: 0.5049383044242859\n",
      "epoch 2, batch 500 \\ 1200: 0.5121594071388245\n",
      "epoch 2, batch 600 \\ 1200: 0.35953179001808167\n",
      "epoch 2, batch 700 \\ 1200: 0.24112775921821594\n",
      "epoch 2, batch 800 \\ 1200: 0.18869592249393463\n",
      "epoch 2, batch 900 \\ 1200: 0.22845330834388733\n",
      "epoch 2, batch 1000 \\ 1200: 0.33125466108322144\n",
      "epoch 2, batch 1100 \\ 1200: 0.07640212774276733\n",
      "epoch 2, batch 1199 \\ 1200: 0.45417892932891846\n",
      "The accuracy:  0.9023\n",
      "epoch 3, batch 0 \\ 1200: 0.25912463665008545\n",
      "epoch 3, batch 100 \\ 1200: 0.47281739115715027\n",
      "epoch 3, batch 200 \\ 1200: 0.36518850922584534\n",
      "epoch 3, batch 300 \\ 1200: 0.4627963602542877\n",
      "epoch 3, batch 400 \\ 1200: 0.36464032530784607\n",
      "epoch 3, batch 500 \\ 1200: 0.3064984381198883\n",
      "epoch 3, batch 600 \\ 1200: 0.14864282310009003\n",
      "epoch 3, batch 700 \\ 1200: 0.7392387390136719\n",
      "epoch 3, batch 800 \\ 1200: 0.3612154424190521\n",
      "epoch 3, batch 900 \\ 1200: 0.3597829341888428\n",
      "epoch 3, batch 1000 \\ 1200: 0.11433670669794083\n",
      "epoch 3, batch 1100 \\ 1200: 0.315746933221817\n",
      "epoch 3, batch 1199 \\ 1200: 0.42345568537712097\n",
      "The accuracy:  0.9219\n",
      "epoch 4, batch 0 \\ 1200: 0.3725682199001312\n",
      "epoch 4, batch 100 \\ 1200: 0.1944556087255478\n",
      "epoch 4, batch 200 \\ 1200: 0.13118930160999298\n",
      "epoch 4, batch 300 \\ 1200: 0.24892576038837433\n",
      "epoch 4, batch 400 \\ 1200: 0.2820932865142822\n",
      "epoch 4, batch 500 \\ 1200: 0.2195064127445221\n",
      "epoch 4, batch 600 \\ 1200: 0.34626781940460205\n",
      "epoch 4, batch 700 \\ 1200: 0.07796178013086319\n",
      "epoch 4, batch 800 \\ 1200: 0.4209127426147461\n",
      "epoch 4, batch 900 \\ 1200: 0.3009847104549408\n",
      "epoch 4, batch 1000 \\ 1200: 0.3643207252025604\n",
      "epoch 4, batch 1100 \\ 1200: 0.18889231979846954\n",
      "epoch 4, batch 1199 \\ 1200: 0.3555406332015991\n",
      "The accuracy:  0.9066\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 5    \n",
    "LR = 0.01\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.01, Batch_Size: 500\n",
      "epoch 0, batch 0 \\ 120: 2.3072257041931152\n",
      "epoch 0, batch 100 \\ 120: 0.380398690700531\n",
      "epoch 0, batch 119 \\ 120: 0.3800472617149353\n",
      "The accuracy:  0.905\n",
      "epoch 1, batch 0 \\ 120: 0.30659493803977966\n",
      "epoch 1, batch 100 \\ 120: 0.31294992566108704\n",
      "epoch 1, batch 119 \\ 120: 0.22828170657157898\n",
      "The accuracy:  0.9295\n",
      "epoch 2, batch 0 \\ 120: 0.2730249762535095\n",
      "epoch 2, batch 100 \\ 120: 0.2213660478591919\n",
      "epoch 2, batch 119 \\ 120: 0.19054743647575378\n",
      "The accuracy:  0.9393\n",
      "epoch 3, batch 0 \\ 120: 0.20591777563095093\n",
      "epoch 3, batch 100 \\ 120: 0.22176162898540497\n",
      "epoch 3, batch 119 \\ 120: 0.22029085457324982\n",
      "The accuracy:  0.9474\n",
      "epoch 4, batch 0 \\ 120: 0.21365687251091003\n",
      "epoch 4, batch 100 \\ 120: 0.15967237949371338\n",
      "epoch 4, batch 119 \\ 120: 0.24157488346099854\n",
      "The accuracy:  0.9425\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 5    \n",
    "LR = 0.01\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Great!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.001, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.3056812286376953\n",
      "epoch 0, batch 100 \\ 1200: 0.9485573768615723\n",
      "epoch 0, batch 200 \\ 1200: 0.4968200922012329\n",
      "epoch 0, batch 300 \\ 1200: 0.5669557452201843\n",
      "epoch 0, batch 400 \\ 1200: 0.40853461623191833\n",
      "epoch 0, batch 500 \\ 1200: 0.36985889077186584\n",
      "epoch 0, batch 600 \\ 1200: 0.3702736794948578\n",
      "epoch 0, batch 700 \\ 1200: 0.5365966558456421\n",
      "epoch 0, batch 800 \\ 1200: 0.24757924675941467\n",
      "epoch 0, batch 900 \\ 1200: 0.3024618625640869\n",
      "epoch 0, batch 1000 \\ 1200: 0.2053707093000412\n",
      "epoch 0, batch 1100 \\ 1200: 0.40047574043273926\n",
      "epoch 0, batch 1199 \\ 1200: 0.4163241982460022\n",
      "The accuracy:  0.9162\n",
      "epoch 1, batch 0 \\ 1200: 0.23867452144622803\n",
      "epoch 1, batch 100 \\ 1200: 0.27230626344680786\n",
      "epoch 1, batch 200 \\ 1200: 0.16738730669021606\n",
      "epoch 1, batch 300 \\ 1200: 0.16350458562374115\n",
      "epoch 1, batch 400 \\ 1200: 0.4388209879398346\n",
      "epoch 1, batch 500 \\ 1200: 0.35524508357048035\n",
      "epoch 1, batch 600 \\ 1200: 0.0945853441953659\n",
      "epoch 1, batch 700 \\ 1200: 0.3480856716632843\n",
      "epoch 1, batch 800 \\ 1200: 0.3353963792324066\n",
      "epoch 1, batch 900 \\ 1200: 0.17623692750930786\n",
      "epoch 1, batch 1000 \\ 1200: 0.3236503303050995\n",
      "epoch 1, batch 1100 \\ 1200: 0.15758495032787323\n",
      "epoch 1, batch 1199 \\ 1200: 0.24229885637760162\n",
      "The accuracy:  0.9372\n",
      "epoch 2, batch 0 \\ 1200: 0.11114393919706345\n",
      "epoch 2, batch 100 \\ 1200: 0.1589222252368927\n",
      "epoch 2, batch 200 \\ 1200: 0.1270250380039215\n",
      "epoch 2, batch 300 \\ 1200: 0.2933473289012909\n",
      "epoch 2, batch 400 \\ 1200: 0.1312669813632965\n",
      "epoch 2, batch 500 \\ 1200: 0.14911678433418274\n",
      "epoch 2, batch 600 \\ 1200: 0.11236608773469925\n",
      "epoch 2, batch 700 \\ 1200: 0.13151243329048157\n",
      "epoch 2, batch 800 \\ 1200: 0.30968403816223145\n",
      "epoch 2, batch 900 \\ 1200: 0.2728320062160492\n",
      "epoch 2, batch 1000 \\ 1200: 0.22926296293735504\n",
      "epoch 2, batch 1100 \\ 1200: 0.2839027941226959\n",
      "epoch 2, batch 1199 \\ 1200: 0.3014470934867859\n",
      "The accuracy:  0.9463\n",
      "epoch 3, batch 0 \\ 1200: 0.1068911999464035\n",
      "epoch 3, batch 100 \\ 1200: 0.17218086123466492\n",
      "epoch 3, batch 200 \\ 1200: 0.08061636984348297\n",
      "epoch 3, batch 300 \\ 1200: 0.22743837535381317\n",
      "epoch 3, batch 400 \\ 1200: 0.08082862198352814\n",
      "epoch 3, batch 500 \\ 1200: 0.2988607883453369\n",
      "epoch 3, batch 600 \\ 1200: 0.08843231946229935\n",
      "epoch 3, batch 700 \\ 1200: 0.26139241456985474\n",
      "epoch 3, batch 800 \\ 1200: 0.1624155044555664\n",
      "epoch 3, batch 900 \\ 1200: 0.23373112082481384\n",
      "epoch 3, batch 1000 \\ 1200: 0.1995871216058731\n",
      "epoch 3, batch 1100 \\ 1200: 0.1650959998369217\n",
      "epoch 3, batch 1199 \\ 1200: 0.2621651589870453\n",
      "The accuracy:  0.947\n",
      "epoch 4, batch 0 \\ 1200: 0.17455708980560303\n",
      "epoch 4, batch 100 \\ 1200: 0.04907782003283501\n",
      "epoch 4, batch 200 \\ 1200: 0.1672486662864685\n",
      "epoch 4, batch 300 \\ 1200: 0.24200795590877533\n",
      "epoch 4, batch 400 \\ 1200: 0.21666426956653595\n",
      "epoch 4, batch 500 \\ 1200: 0.11848703026771545\n",
      "epoch 4, batch 600 \\ 1200: 0.21947042644023895\n",
      "epoch 4, batch 700 \\ 1200: 0.10847938805818558\n",
      "epoch 4, batch 800 \\ 1200: 0.05742508918046951\n",
      "epoch 4, batch 900 \\ 1200: 0.27514412999153137\n",
      "epoch 4, batch 1000 \\ 1200: 0.09911671280860901\n",
      "epoch 4, batch 1100 \\ 1200: 0.0380900613963604\n",
      "epoch 4, batch 1199 \\ 1200: 0.19385918974876404\n",
      "The accuracy:  0.9595\n",
      "epoch 5, batch 0 \\ 1200: 0.13332153856754303\n",
      "epoch 5, batch 100 \\ 1200: 0.2855985164642334\n",
      "epoch 5, batch 200 \\ 1200: 0.06104801222681999\n",
      "epoch 5, batch 300 \\ 1200: 0.07559458911418915\n",
      "epoch 5, batch 400 \\ 1200: 0.13279855251312256\n",
      "epoch 5, batch 500 \\ 1200: 0.2886480987071991\n",
      "epoch 5, batch 600 \\ 1200: 0.1320001184940338\n",
      "epoch 5, batch 700 \\ 1200: 0.111565500497818\n",
      "epoch 5, batch 800 \\ 1200: 0.1914193481206894\n",
      "epoch 5, batch 900 \\ 1200: 0.13003626465797424\n",
      "epoch 5, batch 1000 \\ 1200: 0.04411675035953522\n",
      "epoch 5, batch 1100 \\ 1200: 0.24519082903862\n",
      "epoch 5, batch 1199 \\ 1200: 0.1430789977312088\n",
      "The accuracy:  0.9583\n",
      "epoch 6, batch 0 \\ 1200: 0.22172518074512482\n",
      "epoch 6, batch 100 \\ 1200: 0.39628827571868896\n",
      "epoch 6, batch 200 \\ 1200: 0.17733453214168549\n",
      "epoch 6, batch 300 \\ 1200: 0.2479495406150818\n",
      "epoch 6, batch 400 \\ 1200: 0.02988390438258648\n",
      "epoch 6, batch 500 \\ 1200: 0.041893113404512405\n",
      "epoch 6, batch 600 \\ 1200: 0.09756424278020859\n",
      "epoch 6, batch 700 \\ 1200: 0.08123414218425751\n",
      "epoch 6, batch 800 \\ 1200: 0.12258975207805634\n",
      "epoch 6, batch 900 \\ 1200: 0.06486453115940094\n",
      "epoch 6, batch 1000 \\ 1200: 0.09602227061986923\n",
      "epoch 6, batch 1100 \\ 1200: 0.3273351788520813\n",
      "epoch 6, batch 1199 \\ 1200: 0.0732775554060936\n",
      "The accuracy:  0.9608\n",
      "epoch 7, batch 0 \\ 1200: 0.11746273934841156\n",
      "epoch 7, batch 100 \\ 1200: 0.16650301218032837\n",
      "epoch 7, batch 200 \\ 1200: 0.12597830593585968\n",
      "epoch 7, batch 300 \\ 1200: 0.06839673221111298\n",
      "epoch 7, batch 400 \\ 1200: 0.07167983055114746\n",
      "epoch 7, batch 500 \\ 1200: 0.24198110401630402\n",
      "epoch 7, batch 600 \\ 1200: 0.11641084402799606\n",
      "epoch 7, batch 700 \\ 1200: 0.15190750360488892\n",
      "epoch 7, batch 800 \\ 1200: 0.160508930683136\n",
      "epoch 7, batch 900 \\ 1200: 0.11173858493566513\n",
      "epoch 7, batch 1000 \\ 1200: 0.11083409935235977\n",
      "epoch 7, batch 1100 \\ 1200: 0.14032168686389923\n",
      "epoch 7, batch 1199 \\ 1200: 0.05387166887521744\n",
      "The accuracy:  0.9615\n",
      "epoch 8, batch 0 \\ 1200: 0.1452087163925171\n",
      "epoch 8, batch 100 \\ 1200: 0.181308314204216\n",
      "epoch 8, batch 200 \\ 1200: 0.0527266263961792\n",
      "epoch 8, batch 300 \\ 1200: 0.06702300906181335\n",
      "epoch 8, batch 400 \\ 1200: 0.1467832624912262\n",
      "epoch 8, batch 500 \\ 1200: 0.08519183844327927\n",
      "epoch 8, batch 600 \\ 1200: 0.27780425548553467\n",
      "epoch 8, batch 700 \\ 1200: 0.11419405043125153\n",
      "epoch 8, batch 800 \\ 1200: 0.13728317618370056\n",
      "epoch 8, batch 900 \\ 1200: 0.2517808675765991\n",
      "epoch 8, batch 1000 \\ 1200: 0.08157003670930862\n",
      "epoch 8, batch 1100 \\ 1200: 0.13787995278835297\n",
      "epoch 8, batch 1199 \\ 1200: 0.09497185796499252\n",
      "The accuracy:  0.9632\n",
      "epoch 9, batch 0 \\ 1200: 0.09832122176885605\n",
      "epoch 9, batch 100 \\ 1200: 0.09725223481655121\n",
      "epoch 9, batch 200 \\ 1200: 0.1538466215133667\n",
      "epoch 9, batch 300 \\ 1200: 0.07849724590778351\n",
      "epoch 9, batch 400 \\ 1200: 0.34654128551483154\n",
      "epoch 9, batch 500 \\ 1200: 0.1027245745062828\n",
      "epoch 9, batch 600 \\ 1200: 0.07795944064855576\n",
      "epoch 9, batch 700 \\ 1200: 0.0693475753068924\n",
      "epoch 9, batch 800 \\ 1200: 0.1363208144903183\n",
      "epoch 9, batch 900 \\ 1200: 0.09345735609531403\n",
      "epoch 9, batch 1000 \\ 1200: 0.10391198098659515\n",
      "epoch 9, batch 1100 \\ 1200: 0.32558751106262207\n",
      "epoch 9, batch 1199 \\ 1200: 0.1130264475941658\n",
      "The accuracy:  0.9663\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment, LR: 0.001, Batch_Size: 500\n",
      "epoch 0, batch 0 \\ 120: 2.310904026031494\n",
      "epoch 0, batch 100 \\ 120: 0.620497465133667\n",
      "epoch 0, batch 119 \\ 120: 0.5376012921333313\n",
      "The accuracy:  0.8399\n",
      "epoch 1, batch 0 \\ 120: 0.5342500805854797\n",
      "epoch 1, batch 100 \\ 120: 0.4057137370109558\n",
      "epoch 1, batch 119 \\ 120: 0.43442302942276\n",
      "The accuracy:  0.8925\n",
      "epoch 2, batch 0 \\ 120: 0.45172548294067383\n",
      "epoch 2, batch 100 \\ 120: 0.30685344338417053\n",
      "epoch 2, batch 119 \\ 120: 0.24363884329795837\n",
      "The accuracy:  0.9118\n",
      "epoch 3, batch 0 \\ 120: 0.29189205169677734\n",
      "epoch 3, batch 100 \\ 120: 0.2508179843425751\n",
      "epoch 3, batch 119 \\ 120: 0.32550522685050964\n",
      "The accuracy:  0.9271\n",
      "epoch 4, batch 0 \\ 120: 0.27159178256988525\n",
      "epoch 4, batch 100 \\ 120: 0.2723270654678345\n",
      "epoch 4, batch 119 \\ 120: 0.25161752104759216\n",
      "The accuracy:  0.9343\n",
      "epoch 5, batch 0 \\ 120: 0.2553897202014923\n",
      "epoch 5, batch 100 \\ 120: 0.26446428894996643\n",
      "epoch 5, batch 119 \\ 120: 0.2410522848367691\n",
      "The accuracy:  0.9359\n",
      "epoch 6, batch 0 \\ 120: 0.20878498256206512\n",
      "epoch 6, batch 100 \\ 120: 0.1568485051393509\n",
      "epoch 6, batch 119 \\ 120: 0.24695394933223724\n",
      "The accuracy:  0.9413\n",
      "epoch 7, batch 0 \\ 120: 0.21153387427330017\n",
      "epoch 7, batch 100 \\ 120: 0.2616181969642639\n",
      "epoch 7, batch 119 \\ 120: 0.20906831324100494\n",
      "The accuracy:  0.9441\n",
      "epoch 8, batch 0 \\ 120: 0.1896662414073944\n",
      "epoch 8, batch 100 \\ 120: 0.2054489403963089\n",
      "epoch 8, batch 119 \\ 120: 0.2310314029455185\n",
      "The accuracy:  0.9457\n",
      "epoch 9, batch 0 \\ 120: 0.2372266948223114\n",
      "epoch 9, batch 100 \\ 120: 0.1817929744720459\n",
      "epoch 9, batch 119 \\ 120: 0.2558235824108124\n",
      "The accuracy:  0.9482\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10   \n",
    "LR = 0.001\n",
    "BATCH_SIZE = 500\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Experiment, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 加深网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (convfirst): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (convfinal): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convfinal_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Deep+5, LR: 0.001, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.2912659645080566\n",
      "epoch 0, batch 100 \\ 1200: 1.5827502012252808\n",
      "epoch 0, batch 200 \\ 1200: 1.0588760375976562\n",
      "epoch 0, batch 300 \\ 1200: 0.4336460530757904\n",
      "epoch 0, batch 400 \\ 1200: 0.6408582329750061\n",
      "epoch 0, batch 500 \\ 1200: 0.4688640534877777\n",
      "epoch 0, batch 600 \\ 1200: 0.6053979396820068\n",
      "epoch 0, batch 700 \\ 1200: 0.21313819289207458\n",
      "epoch 0, batch 800 \\ 1200: 0.2772435247898102\n",
      "epoch 0, batch 900 \\ 1200: 0.215111643075943\n",
      "epoch 0, batch 1000 \\ 1200: 0.27541351318359375\n",
      "epoch 0, batch 1100 \\ 1200: 0.28735992312431335\n",
      "epoch 0, batch 1199 \\ 1200: 0.18904979526996613\n",
      "The accuracy:  0.9189\n",
      "epoch 1, batch 0 \\ 1200: 0.1235896423459053\n",
      "epoch 1, batch 100 \\ 1200: 0.1948080211877823\n",
      "epoch 1, batch 200 \\ 1200: 0.16733045876026154\n",
      "epoch 1, batch 300 \\ 1200: 0.2673996090888977\n",
      "epoch 1, batch 400 \\ 1200: 0.1399272084236145\n",
      "epoch 1, batch 500 \\ 1200: 0.15040108561515808\n",
      "epoch 1, batch 600 \\ 1200: 0.3676261603832245\n",
      "epoch 1, batch 700 \\ 1200: 0.22078841924667358\n",
      "epoch 1, batch 800 \\ 1200: 0.2226780652999878\n",
      "epoch 1, batch 900 \\ 1200: 0.38414904475212097\n",
      "epoch 1, batch 1000 \\ 1200: 0.2178652435541153\n",
      "epoch 1, batch 1100 \\ 1200: 0.12290937453508377\n",
      "epoch 1, batch 1199 \\ 1200: 0.09380026161670685\n",
      "The accuracy:  0.9413\n",
      "epoch 2, batch 0 \\ 1200: 0.3298496901988983\n",
      "epoch 2, batch 100 \\ 1200: 0.1444774717092514\n",
      "epoch 2, batch 200 \\ 1200: 0.3471442759037018\n",
      "epoch 2, batch 300 \\ 1200: 0.37505584955215454\n",
      "epoch 2, batch 400 \\ 1200: 0.10564616322517395\n",
      "epoch 2, batch 500 \\ 1200: 0.1466967612504959\n",
      "epoch 2, batch 600 \\ 1200: 0.46541664004325867\n",
      "epoch 2, batch 700 \\ 1200: 0.2500198483467102\n",
      "epoch 2, batch 800 \\ 1200: 0.15618692338466644\n",
      "epoch 2, batch 900 \\ 1200: 0.46845927834510803\n",
      "epoch 2, batch 1000 \\ 1200: 0.17549972236156464\n",
      "epoch 2, batch 1100 \\ 1200: 0.18291164934635162\n",
      "epoch 2, batch 1199 \\ 1200: 0.20049279928207397\n",
      "The accuracy:  0.9517\n",
      "epoch 3, batch 0 \\ 1200: 0.3042508065700531\n",
      "epoch 3, batch 100 \\ 1200: 0.07520514726638794\n",
      "epoch 3, batch 200 \\ 1200: 0.132772758603096\n",
      "epoch 3, batch 300 \\ 1200: 0.1974073052406311\n",
      "epoch 3, batch 400 \\ 1200: 0.33787915110588074\n",
      "epoch 3, batch 500 \\ 1200: 0.06580705940723419\n",
      "epoch 3, batch 600 \\ 1200: 0.08422575145959854\n",
      "epoch 3, batch 700 \\ 1200: 0.09562196582555771\n",
      "epoch 3, batch 800 \\ 1200: 0.1570519208908081\n",
      "epoch 3, batch 900 \\ 1200: 0.014897381886839867\n",
      "epoch 3, batch 1000 \\ 1200: 0.316165953874588\n",
      "epoch 3, batch 1100 \\ 1200: 0.07307499647140503\n",
      "epoch 3, batch 1199 \\ 1200: 0.17252351343631744\n",
      "The accuracy:  0.9612\n",
      "epoch 4, batch 0 \\ 1200: 0.13576263189315796\n",
      "epoch 4, batch 100 \\ 1200: 0.15364386141300201\n",
      "epoch 4, batch 200 \\ 1200: 0.018742449581623077\n",
      "epoch 4, batch 300 \\ 1200: 0.3483447730541229\n",
      "epoch 4, batch 400 \\ 1200: 0.032703250646591187\n",
      "epoch 4, batch 500 \\ 1200: 0.10651417821645737\n",
      "epoch 4, batch 600 \\ 1200: 0.15175873041152954\n",
      "epoch 4, batch 700 \\ 1200: 0.1603834331035614\n",
      "epoch 4, batch 800 \\ 1200: 0.21078716218471527\n",
      "epoch 4, batch 900 \\ 1200: 0.24581879377365112\n",
      "epoch 4, batch 1000 \\ 1200: 0.05887451767921448\n",
      "epoch 4, batch 1100 \\ 1200: 0.09236005693674088\n",
      "epoch 4, batch 1199 \\ 1200: 0.10311722755432129\n",
      "The accuracy:  0.9649\n",
      "epoch 5, batch 0 \\ 1200: 0.03944331780076027\n",
      "epoch 5, batch 100 \\ 1200: 0.13055752217769623\n",
      "epoch 5, batch 200 \\ 1200: 0.17258086800575256\n",
      "epoch 5, batch 300 \\ 1200: 0.05910395085811615\n",
      "epoch 5, batch 400 \\ 1200: 0.11293283104896545\n",
      "epoch 5, batch 500 \\ 1200: 0.14451374113559723\n",
      "epoch 5, batch 600 \\ 1200: 0.26921191811561584\n",
      "epoch 5, batch 700 \\ 1200: 0.10015103220939636\n",
      "epoch 5, batch 800 \\ 1200: 0.2851354777812958\n",
      "epoch 5, batch 900 \\ 1200: 0.19969095289707184\n",
      "epoch 5, batch 1000 \\ 1200: 0.05855061113834381\n",
      "epoch 5, batch 1100 \\ 1200: 0.06650923937559128\n",
      "epoch 5, batch 1199 \\ 1200: 0.3046441376209259\n",
      "The accuracy:  0.9644\n",
      "epoch 6, batch 0 \\ 1200: 0.054310813546180725\n",
      "epoch 6, batch 100 \\ 1200: 0.16777195036411285\n",
      "epoch 6, batch 200 \\ 1200: 0.04585263133049011\n",
      "epoch 6, batch 300 \\ 1200: 0.08545717597007751\n",
      "epoch 6, batch 400 \\ 1200: 0.02669607847929001\n",
      "epoch 6, batch 500 \\ 1200: 0.07481233030557632\n",
      "epoch 6, batch 600 \\ 1200: 0.1791437715291977\n",
      "epoch 6, batch 700 \\ 1200: 0.03387974202632904\n",
      "epoch 6, batch 800 \\ 1200: 0.01279106643050909\n",
      "epoch 6, batch 900 \\ 1200: 0.15297159552574158\n",
      "epoch 6, batch 1000 \\ 1200: 0.15490713715553284\n",
      "epoch 6, batch 1100 \\ 1200: 0.10598897933959961\n",
      "epoch 6, batch 1199 \\ 1200: 0.21631525456905365\n",
      "The accuracy:  0.9675\n",
      "epoch 7, batch 0 \\ 1200: 0.11977523565292358\n",
      "epoch 7, batch 100 \\ 1200: 0.10358819365501404\n",
      "epoch 7, batch 200 \\ 1200: 0.0868685320019722\n",
      "epoch 7, batch 300 \\ 1200: 0.02833031676709652\n",
      "epoch 7, batch 400 \\ 1200: 0.012205414474010468\n",
      "epoch 7, batch 500 \\ 1200: 0.11157426983118057\n",
      "epoch 7, batch 600 \\ 1200: 0.14021091163158417\n",
      "epoch 7, batch 700 \\ 1200: 0.0685410276055336\n",
      "epoch 7, batch 800 \\ 1200: 0.09045913815498352\n",
      "epoch 7, batch 900 \\ 1200: 0.0286520067602396\n",
      "epoch 7, batch 1000 \\ 1200: 0.11135168373584747\n",
      "epoch 7, batch 1100 \\ 1200: 0.1270291656255722\n",
      "epoch 7, batch 1199 \\ 1200: 0.08149617910385132\n",
      "The accuracy:  0.97\n",
      "epoch 8, batch 0 \\ 1200: 0.20743410289287567\n",
      "epoch 8, batch 100 \\ 1200: 0.09053555130958557\n",
      "epoch 8, batch 200 \\ 1200: 0.06441820412874222\n",
      "epoch 8, batch 300 \\ 1200: 0.03674549236893654\n",
      "epoch 8, batch 400 \\ 1200: 0.08156003803014755\n",
      "epoch 8, batch 500 \\ 1200: 0.025429684668779373\n",
      "epoch 8, batch 600 \\ 1200: 0.20120082795619965\n",
      "epoch 8, batch 700 \\ 1200: 0.1332412213087082\n",
      "epoch 8, batch 800 \\ 1200: 0.022792387753725052\n",
      "epoch 8, batch 900 \\ 1200: 0.09598853439092636\n",
      "epoch 8, batch 1000 \\ 1200: 0.12028061598539352\n",
      "epoch 8, batch 1100 \\ 1200: 0.17507271468639374\n",
      "epoch 8, batch 1199 \\ 1200: 0.12033002823591232\n",
      "The accuracy:  0.9723\n",
      "epoch 9, batch 0 \\ 1200: 0.11185920983552933\n",
      "epoch 9, batch 100 \\ 1200: 0.06014876067638397\n",
      "epoch 9, batch 200 \\ 1200: 0.011966180987656116\n",
      "epoch 9, batch 300 \\ 1200: 0.2314574420452118\n",
      "epoch 9, batch 400 \\ 1200: 0.049574073404073715\n",
      "epoch 9, batch 500 \\ 1200: 0.28380638360977173\n",
      "epoch 9, batch 600 \\ 1200: 0.039107635617256165\n",
      "epoch 9, batch 700 \\ 1200: 0.11313516646623611\n",
      "epoch 9, batch 800 \\ 1200: 0.005470560397952795\n",
      "epoch 9, batch 900 \\ 1200: 0.016846688464283943\n",
      "epoch 9, batch 1000 \\ 1200: 0.11997263133525848\n",
      "epoch 9, batch 1100 \\ 1200: 0.034219201654195786\n",
      "epoch 9, batch 1199 \\ 1200: 0.03193066269159317\n",
      "The accuracy:  0.9749\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.convfirst = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.convfinal = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convfinal_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convfirst(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convfinal(x)\n",
    "        x = F.relu(F.max_pool2d(self.convfinal_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = DeepNet()\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Deep+5, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (convfirst): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (convfinal): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convfinal_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Deep+10, LR: 0.001, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.3045737743377686\n",
      "epoch 0, batch 100 \\ 1200: 2.304295539855957\n",
      "epoch 0, batch 200 \\ 1200: 1.3523430824279785\n",
      "epoch 0, batch 300 \\ 1200: 1.0528903007507324\n",
      "epoch 0, batch 400 \\ 1200: 1.099544644355774\n",
      "epoch 0, batch 500 \\ 1200: 0.791851818561554\n",
      "epoch 0, batch 600 \\ 1200: 0.7360271215438843\n",
      "epoch 0, batch 700 \\ 1200: 0.6642230153083801\n",
      "epoch 0, batch 800 \\ 1200: 0.44145092368125916\n",
      "epoch 0, batch 900 \\ 1200: 0.23158462345600128\n",
      "epoch 0, batch 1000 \\ 1200: 0.491159051656723\n",
      "epoch 0, batch 1100 \\ 1200: 0.41980698704719543\n",
      "epoch 0, batch 1199 \\ 1200: 0.5829994678497314\n",
      "The accuracy:  0.8767\n",
      "epoch 1, batch 0 \\ 1200: 0.48615288734436035\n",
      "epoch 1, batch 100 \\ 1200: 0.3962564766407013\n",
      "epoch 1, batch 200 \\ 1200: 0.41882506012916565\n",
      "epoch 1, batch 300 \\ 1200: 0.22117295861244202\n",
      "epoch 1, batch 400 \\ 1200: 0.1842801719903946\n",
      "epoch 1, batch 500 \\ 1200: 0.2896054983139038\n",
      "epoch 1, batch 600 \\ 1200: 0.5724617838859558\n",
      "epoch 1, batch 700 \\ 1200: 0.3864095211029053\n",
      "epoch 1, batch 800 \\ 1200: 0.4199158549308777\n",
      "epoch 1, batch 900 \\ 1200: 0.24871191382408142\n",
      "epoch 1, batch 1000 \\ 1200: 0.31758517026901245\n",
      "epoch 1, batch 1100 \\ 1200: 0.49858683347702026\n",
      "epoch 1, batch 1199 \\ 1200: 0.23400291800498962\n",
      "The accuracy:  0.9338\n",
      "epoch 2, batch 0 \\ 1200: 0.2954705059528351\n",
      "epoch 2, batch 100 \\ 1200: 0.14532633125782013\n",
      "epoch 2, batch 200 \\ 1200: 0.12530089914798737\n",
      "epoch 2, batch 300 \\ 1200: 0.19653168320655823\n",
      "epoch 2, batch 400 \\ 1200: 0.2579512894153595\n",
      "epoch 2, batch 500 \\ 1200: 0.05900533124804497\n",
      "epoch 2, batch 600 \\ 1200: 0.3875071406364441\n",
      "epoch 2, batch 700 \\ 1200: 0.2755715548992157\n",
      "epoch 2, batch 800 \\ 1200: 0.26467618346214294\n",
      "epoch 2, batch 900 \\ 1200: 0.17885065078735352\n",
      "epoch 2, batch 1000 \\ 1200: 0.4103105068206787\n",
      "epoch 2, batch 1100 \\ 1200: 0.16581977903842926\n",
      "epoch 2, batch 1199 \\ 1200: 0.4603375196456909\n",
      "The accuracy:  0.9463\n",
      "epoch 3, batch 0 \\ 1200: 0.26384150981903076\n",
      "epoch 3, batch 100 \\ 1200: 0.09279721975326538\n",
      "epoch 3, batch 200 \\ 1200: 0.2912537753582001\n",
      "epoch 3, batch 300 \\ 1200: 0.466146320104599\n",
      "epoch 3, batch 400 \\ 1200: 0.17713962495326996\n",
      "epoch 3, batch 500 \\ 1200: 0.2728350758552551\n",
      "epoch 3, batch 600 \\ 1200: 0.6262657046318054\n",
      "epoch 3, batch 700 \\ 1200: 0.08703967928886414\n",
      "epoch 3, batch 800 \\ 1200: 0.16664323210716248\n",
      "epoch 3, batch 900 \\ 1200: 0.144259974360466\n",
      "epoch 3, batch 1000 \\ 1200: 0.25528737902641296\n",
      "epoch 3, batch 1100 \\ 1200: 0.0702330619096756\n",
      "epoch 3, batch 1199 \\ 1200: 0.16649337112903595\n",
      "The accuracy:  0.962\n",
      "epoch 4, batch 0 \\ 1200: 0.10822726041078568\n",
      "epoch 4, batch 100 \\ 1200: 0.1436513066291809\n",
      "epoch 4, batch 200 \\ 1200: 0.19135960936546326\n",
      "epoch 4, batch 300 \\ 1200: 0.2576260268688202\n",
      "epoch 4, batch 400 \\ 1200: 0.15203255414962769\n",
      "epoch 4, batch 500 \\ 1200: 0.17246025800704956\n",
      "epoch 4, batch 600 \\ 1200: 0.04770481213927269\n",
      "epoch 4, batch 700 \\ 1200: 0.15945996344089508\n",
      "epoch 4, batch 800 \\ 1200: 0.19223757088184357\n",
      "epoch 4, batch 900 \\ 1200: 0.07842027395963669\n",
      "epoch 4, batch 1000 \\ 1200: 0.14788489043712616\n",
      "epoch 4, batch 1100 \\ 1200: 0.09916364401578903\n",
      "epoch 4, batch 1199 \\ 1200: 0.17666319012641907\n",
      "The accuracy:  0.9645\n",
      "epoch 5, batch 0 \\ 1200: 0.06916525214910507\n",
      "epoch 5, batch 100 \\ 1200: 0.04026470333337784\n",
      "epoch 5, batch 200 \\ 1200: 0.12204598635435104\n",
      "epoch 5, batch 300 \\ 1200: 0.11873091757297516\n",
      "epoch 5, batch 400 \\ 1200: 0.10026123374700546\n",
      "epoch 5, batch 500 \\ 1200: 0.1710006147623062\n",
      "epoch 5, batch 600 \\ 1200: 0.4810345768928528\n",
      "epoch 5, batch 700 \\ 1200: 0.06147439032793045\n",
      "epoch 5, batch 800 \\ 1200: 0.09663519263267517\n",
      "epoch 5, batch 900 \\ 1200: 0.19535507261753082\n",
      "epoch 5, batch 1000 \\ 1200: 0.14429470896720886\n",
      "epoch 5, batch 1100 \\ 1200: 0.2865687310695648\n",
      "epoch 5, batch 1199 \\ 1200: 0.07760929316282272\n",
      "The accuracy:  0.9623\n",
      "epoch 6, batch 0 \\ 1200: 0.1363092064857483\n",
      "epoch 6, batch 100 \\ 1200: 0.08974974602460861\n",
      "epoch 6, batch 200 \\ 1200: 0.03210630267858505\n",
      "epoch 6, batch 300 \\ 1200: 0.16007253527641296\n",
      "epoch 6, batch 400 \\ 1200: 0.11403939127922058\n",
      "epoch 6, batch 500 \\ 1200: 0.045087847858667374\n",
      "epoch 6, batch 600 \\ 1200: 0.24282409250736237\n",
      "epoch 6, batch 700 \\ 1200: 0.057268571108579636\n",
      "epoch 6, batch 800 \\ 1200: 0.12162478268146515\n",
      "epoch 6, batch 900 \\ 1200: 0.3615071773529053\n",
      "epoch 6, batch 1000 \\ 1200: 0.06790174543857574\n",
      "epoch 6, batch 1100 \\ 1200: 0.03421381860971451\n",
      "epoch 6, batch 1199 \\ 1200: 0.07282853871583939\n",
      "The accuracy:  0.9748\n",
      "epoch 7, batch 0 \\ 1200: 0.06657489389181137\n",
      "epoch 7, batch 100 \\ 1200: 0.2362278550863266\n",
      "epoch 7, batch 200 \\ 1200: 0.02871965803205967\n",
      "epoch 7, batch 300 \\ 1200: 0.1557941436767578\n",
      "epoch 7, batch 400 \\ 1200: 0.2137959599494934\n",
      "epoch 7, batch 500 \\ 1200: 0.049565233290195465\n",
      "epoch 7, batch 600 \\ 1200: 0.07056926935911179\n",
      "epoch 7, batch 700 \\ 1200: 0.15815211832523346\n",
      "epoch 7, batch 800 \\ 1200: 0.09133525937795639\n",
      "epoch 7, batch 900 \\ 1200: 0.14147482812404633\n",
      "epoch 7, batch 1000 \\ 1200: 0.08376098424196243\n",
      "epoch 7, batch 1100 \\ 1200: 0.21406923234462738\n",
      "epoch 7, batch 1199 \\ 1200: 0.08893254399299622\n",
      "The accuracy:  0.9731\n",
      "epoch 8, batch 0 \\ 1200: 0.07281086593866348\n",
      "epoch 8, batch 100 \\ 1200: 0.074040986597538\n",
      "epoch 8, batch 200 \\ 1200: 0.040524691343307495\n",
      "epoch 8, batch 300 \\ 1200: 0.2175847887992859\n",
      "epoch 8, batch 400 \\ 1200: 0.030105136334896088\n",
      "epoch 8, batch 500 \\ 1200: 0.10699798911809921\n",
      "epoch 8, batch 600 \\ 1200: 0.08635453879833221\n",
      "epoch 8, batch 700 \\ 1200: 0.06143079698085785\n",
      "epoch 8, batch 800 \\ 1200: 0.1623893678188324\n",
      "epoch 8, batch 900 \\ 1200: 0.04916353523731232\n",
      "epoch 8, batch 1000 \\ 1200: 0.3112480342388153\n",
      "epoch 8, batch 1100 \\ 1200: 0.027640242129564285\n",
      "epoch 8, batch 1199 \\ 1200: 0.05854473263025284\n",
      "The accuracy:  0.9749\n",
      "epoch 9, batch 0 \\ 1200: 0.06470273435115814\n",
      "epoch 9, batch 100 \\ 1200: 0.02321755327284336\n",
      "epoch 9, batch 200 \\ 1200: 0.06488001346588135\n",
      "epoch 9, batch 300 \\ 1200: 0.10499479621648788\n",
      "epoch 9, batch 400 \\ 1200: 0.3677053451538086\n",
      "epoch 9, batch 500 \\ 1200: 0.22347354888916016\n",
      "epoch 9, batch 600 \\ 1200: 0.0708141028881073\n",
      "epoch 9, batch 700 \\ 1200: 0.10006751120090485\n",
      "epoch 9, batch 800 \\ 1200: 0.03360412269830704\n",
      "epoch 9, batch 900 \\ 1200: 0.028731467202305794\n",
      "epoch 9, batch 1000 \\ 1200: 0.09418753534555435\n",
      "epoch 9, batch 1100 \\ 1200: 0.039679042994976044\n",
      "epoch 9, batch 1199 \\ 1200: 0.04659714177250862\n",
      "The accuracy:  0.9756\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 10\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.convfirst = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.convfinal = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convfinal_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convfirst(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convfinal(x)\n",
    "        x = F.relu(F.max_pool2d(self.convfinal_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = DeepNet()\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Deep+10, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (convfirst): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv11): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv13): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv14): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv15): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv16): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv17): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv18): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv19): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv20): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (convfinal): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convfinal_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Deep+20, LR: 0.001, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.309195041656494\n",
      "epoch 0, batch 100 \\ 1200: 2.3249423503875732\n",
      "epoch 0, batch 200 \\ 1200: 2.3106472492218018\n",
      "epoch 0, batch 300 \\ 1200: 2.3044846057891846\n",
      "epoch 0, batch 400 \\ 1200: 2.2895314693450928\n",
      "epoch 0, batch 500 \\ 1200: 2.2947545051574707\n",
      "epoch 0, batch 600 \\ 1200: 2.3137688636779785\n",
      "epoch 0, batch 700 \\ 1200: 2.293260097503662\n",
      "epoch 0, batch 800 \\ 1200: 2.298954725265503\n",
      "epoch 0, batch 900 \\ 1200: 2.299560308456421\n",
      "epoch 0, batch 1000 \\ 1200: 2.301307439804077\n",
      "epoch 0, batch 1100 \\ 1200: 2.2862420082092285\n",
      "epoch 0, batch 1199 \\ 1200: 2.2967731952667236\n",
      "The accuracy:  0.1135\n",
      "epoch 1, batch 0 \\ 1200: 2.30694317817688\n",
      "epoch 1, batch 100 \\ 1200: 2.3020567893981934\n",
      "epoch 1, batch 200 \\ 1200: 2.3005874156951904\n",
      "epoch 1, batch 300 \\ 1200: 2.3083503246307373\n",
      "epoch 1, batch 400 \\ 1200: 2.3087940216064453\n",
      "epoch 1, batch 500 \\ 1200: 2.306997060775757\n",
      "epoch 1, batch 600 \\ 1200: 2.300869941711426\n",
      "epoch 1, batch 700 \\ 1200: 2.3043782711029053\n",
      "epoch 1, batch 800 \\ 1200: 2.3146824836730957\n",
      "epoch 1, batch 900 \\ 1200: 2.3111634254455566\n",
      "epoch 1, batch 1000 \\ 1200: 2.2973639965057373\n",
      "epoch 1, batch 1100 \\ 1200: 2.3081727027893066\n",
      "epoch 1, batch 1199 \\ 1200: 2.295342206954956\n",
      "The accuracy:  0.1135\n",
      "epoch 2, batch 0 \\ 1200: 2.304351329803467\n",
      "epoch 2, batch 100 \\ 1200: 2.2854692935943604\n",
      "epoch 2, batch 200 \\ 1200: 2.3062515258789062\n",
      "epoch 2, batch 300 \\ 1200: 2.2944397926330566\n",
      "epoch 2, batch 400 \\ 1200: 2.301833152770996\n",
      "epoch 2, batch 500 \\ 1200: 2.2976558208465576\n",
      "epoch 2, batch 600 \\ 1200: 2.330056667327881\n",
      "epoch 2, batch 700 \\ 1200: 2.304375171661377\n",
      "epoch 2, batch 800 \\ 1200: 2.2929391860961914\n",
      "epoch 2, batch 900 \\ 1200: 2.2762999534606934\n",
      "epoch 2, batch 1000 \\ 1200: 2.317450761795044\n",
      "epoch 2, batch 1100 \\ 1200: 2.3029236793518066\n",
      "epoch 2, batch 1199 \\ 1200: 2.3017475605010986\n",
      "The accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 3\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.convfirst = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv14 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv15 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv16 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv17 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv18 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv19 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv20 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.convfinal = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convfinal_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convfirst(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv13(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv14(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv15(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv16(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv17(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv18(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv19(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv20(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convfinal(x)\n",
    "        x = F.relu(F.max_pool2d(self.convfinal_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = DeepNet()\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Deep+20, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (convfirst): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv11): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv13): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv14): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv15): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv16): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv17): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv18): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv19): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv20): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (convfinal): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convfinal_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Deep+20, LR: 0.0001, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.2890477180480957\n",
      "epoch 0, batch 100 \\ 1200: 2.29658842086792\n",
      "epoch 0, batch 200 \\ 1200: 2.28761887550354\n",
      "epoch 0, batch 300 \\ 1200: 2.3137125968933105\n",
      "epoch 0, batch 400 \\ 1200: 2.307187557220459\n",
      "epoch 0, batch 500 \\ 1200: 2.3092641830444336\n",
      "epoch 0, batch 600 \\ 1200: 2.3177335262298584\n",
      "epoch 0, batch 700 \\ 1200: 2.2895288467407227\n",
      "epoch 0, batch 800 \\ 1200: 2.3059966564178467\n",
      "epoch 0, batch 900 \\ 1200: 2.2941465377807617\n",
      "epoch 0, batch 1000 \\ 1200: 2.3165624141693115\n",
      "epoch 0, batch 1100 \\ 1200: 2.310730218887329\n",
      "epoch 0, batch 1199 \\ 1200: 2.2987782955169678\n",
      "The accuracy:  0.1082\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1\n",
    "LR = 0.0001\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.convfirst = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv14 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv15 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv16 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv17 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv18 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv19 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv20 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.convfinal = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convfinal_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convfirst(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv13(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv14(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv15(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv16(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv17(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv18(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv19(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv20(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convfinal(x)\n",
    "        x = F.relu(F.max_pool2d(self.convfinal_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = DeepNet()\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Deep+20, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepNet(\n",
      "  (convfirst): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv1): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv7): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv9): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv10): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv11): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv12): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv13): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv14): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv15): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv16): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv17): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv18): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv19): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv20): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (convfinal): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (convfinal_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n",
      "Deep+20, LR: 0.01, Batch_Size: 50\n",
      "epoch 0, batch 0 \\ 1200: 2.296905279159546\n",
      "epoch 0, batch 100 \\ 1200: 2.331209182739258\n",
      "epoch 0, batch 200 \\ 1200: 2.305983304977417\n",
      "epoch 0, batch 300 \\ 1200: 2.309541940689087\n",
      "epoch 0, batch 400 \\ 1200: 2.2947616577148438\n",
      "epoch 0, batch 500 \\ 1200: 2.3180923461914062\n",
      "epoch 0, batch 600 \\ 1200: 2.326106309890747\n",
      "epoch 0, batch 700 \\ 1200: 2.3234031200408936\n",
      "epoch 0, batch 800 \\ 1200: 2.287726402282715\n",
      "epoch 0, batch 900 \\ 1200: 2.3165619373321533\n",
      "epoch 0, batch 1000 \\ 1200: 2.280695915222168\n",
      "epoch 0, batch 1100 \\ 1200: 2.301832675933838\n",
      "epoch 0, batch 1199 \\ 1200: 2.292034864425659\n",
      "The accuracy:  0.1135\n"
     ]
    }
   ],
   "source": [
    "EPOCH = 1\n",
    "LR = 0.01\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# train_loader = Data.DataLoader(dataset=train_data, batch_size=50, shuffle=True)\n",
    "\n",
    "class DeepNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DeepNet, self).__init__()\n",
    "\n",
    "        self.convfirst = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv6 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv7 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv8 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv9 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv10 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv11 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv12 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv13 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv14 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv15 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv16 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv17 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv18 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv19 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv20 = nn.Conv2d(10, 10, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.convfinal = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.convfinal_drop = nn.Dropout2d()\n",
    "\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convfirst(x)\n",
    "        x = F.relu(F.max_pool2d(x, 2))\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv6(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv7(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv8(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv9(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv10(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv11(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv12(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv13(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv14(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv15(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv16(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv17(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv18(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv19(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.conv20(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.convfinal(x)\n",
    "        x = F.relu(F.max_pool2d(self.convfinal_drop(x), 2))\n",
    "\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net = DeepNet()\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Deep+20, LR: {}, Batch_Size: {}\".format(LR, BATCH_SIZE))\n",
    "\n",
    "# training and testing\n",
    "for epoch in range(EPOCH):\n",
    "    for step, (b_x, b_y) in enumerate(train_loader): \n",
    "        output = net(b_x)\n",
    "        loss = loss_func(output, b_y)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "        \n",
    "        if step == len(train_loader) - 1:\n",
    "            print(\"epoch {}, batch {} \\\\ {}: {}\".format(epoch, step, len(train_loader), loss))\n",
    "            \n",
    "    if epoch % 1 == 0:\n",
    "        test_output = net(test_x)\n",
    "        pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "        accuracy = get_accuracy(pred_y, test_y)\n",
    "        print(\"The accuracy: \", accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 深度网络的退化问题\n",
    "从经验来看，网络的深度对模型的性能至关重要, 当增加网络层数后, 网络可以进行更加复杂的特征模式的提取, 所以当模型更深时理论上可以取得更好的结果. \n",
    "\n",
    "实验发现深度网络出现了退化问题(Degradation problem): 网络深度增加时, 网络准确度出现饱和, 甚至出现下降. 这个现象可以在图3中直观看出来: 56层的网络比20层网络效果还要差. 这不会是过拟合问题, 因为56层网络的训练误差同样高. \n",
    "\n",
    "<img src=\"./picture/resnet_1.png\" width=800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度网络的退化问题至少说明深度网络不容易训练。但是我们考虑这样一个事实：现在有一个浅层网络，通过向上堆积新层来建立深层网络，一个极端情况是这些增加的层什么也不学习，仅仅复制浅层网络的特征，即这样新层是恒等映射（Identity mapping）。\n",
    "\n",
    "在这种情况下，深层网络应该至少和浅层网络性能一样，也不应该出现退化现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个有趣的假设让何博士灵感爆发，他提出了残差学习来解决退化问题。对于一个堆积层结构（几层堆积而成）当输入为 $x$ 时其学习到的特征记为$H(x)$ ，现在我们希望其可以学习到残差  $F(x)=H(x)-x$，这样其实原始的学习特征是 $F(x)+x$ 。之所以这样是因为残差学习相比原始特征直接学习更容易。当残差为0时，此时堆积层仅仅做了恒等映射，至少网络性能不会下降，实际上残差不会为0，这也会使得堆积层在输入特征基础上学习到新的特征，从而拥有更好的性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./picture/resnet_2.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么残差学习相对更容易，从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学 习难度小点。不过我们可以从数学的角度来分析这个问题，首先残差单元可以表示为：\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&y_{l}=h\\left(x_{l}\\right)+F\\left(x_{l}, W_{l}\\right) \\\\\n",
    "&x_{l+1}=f\\left(y_{l}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中 $x_{l}$ 和 $x_{l+1}$ 分别表示的是第 $l$ 个残差单元的输入和输出，注意每个残差单元一般包含多层结 构。 $F$ 是残差函数， 表示学习到的残差，而 $h\\left(x_{l}\\right)=x_{l}$ 表示恒等映射, $f$ 是ReLU激活函数。 基于上式, 我们求得从浅层 $l$ 到深层 $L$ 的学习特征为:\n",
    "$$\n",
    "x_{L}=x_{l}+\\sum_{i=l}^{L-1} F\\left(x_{i}, W_{i}\\right)\n",
    "$$\n",
    "利用链式规则，可以求得反向过程的梯度：\n",
    "$\\frac{\\partial l o s s}{\\partial x_{l}}=\\frac{\\partial \\operatorname{loss}}{\\partial x_{L}} \\cdot \\frac{\\partial x_{L}}{\\partial x_{l}}=\\frac{\\partial l o s s}{\\partial x_{L}} \\cdot\\left(1+\\frac{\\partial}{\\partial x_{L}} \\sum_{i=l}^{L-1} F\\left(x_{i}, W_{i}\\right)\\right)$\n",
    "式子的第一个因子 $\\frac{\\partial l o s s}{\\partial x_{L}}$ 表示的损失函数到达 $L$ 的梯度，小括号中的1表明短路机制可以无损地 传播梯度，而另外一项残差梯度则需要经过带有weights的层，梯度不是直接传递过来的。残差梯 度不会那么巧全为 $-1$ ，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容 易。要注意上面的推导并不是严格的证明。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./picture/resnet_3.jpg\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./picture/resnet_4.png\" width=600>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 用于ResNet18和34的残差块，用的是2个3x3的卷积\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        # 经过处理后的x要与x的维度相同(尺寸和深度)\n",
    "        # 如果不相同，需要添加卷积+BN来变换为同一维度\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# 用于ResNet50,101和152的残差块，用的是1x1+3x3+1x1的卷积\n",
    "class Bottleneck(nn.Module):\n",
    "    # 前面1x1和3x3卷积的filter个数相等，最后1x1卷积是其expansion倍\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes,\n",
    "                               kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2,2,2,2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3,4,6,3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3,4,6,3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3,4,23,3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3,8,36,3])\n",
    "\n",
    "\n",
    "def test():\n",
    "    net = ResNet18()\n",
    "    y = net(torch.randn(1,3,32,32))\n",
    "    print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.7.1+cu110\n",
      "Torchvision Version:  0.8.2+cu110\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os, sys\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "# Hyper-parameters\n",
    "gpu_flag = True\n",
    "input_size = 224\n",
    "num_epochs = 3\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "if gpu_flag:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "else:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "    \n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def get_accuracy(pred_y, test_y):\n",
    "    accuracy = 0\n",
    "    for i in range(len(test_y)):\n",
    "        if pred_y[i] == test_y[i]:\n",
    "            accuracy += 1\n",
    "    accuracy /= len(test_y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image preprocessing modules\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Mnist 手写数字\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=data_transforms,\n",
    "                                             download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=data_transforms)\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 网络加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet18(pretrained=False)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 损失函数和优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0/1200], Time 0.09721040725708008\n",
      "Epoch [1/3], Step [100/1200] Loss: 0.1729\n",
      "Batch [100/1200], Time 0.03025531768798828\n",
      "Epoch [1/3], Step [200/1200] Loss: 0.2664\n",
      "Batch [200/1200], Time 0.03590130805969238\n",
      "Epoch [1/3], Step [300/1200] Loss: 0.4628\n",
      "Batch [300/1200], Time 0.03324699401855469\n",
      "Epoch [1/3], Step [400/1200] Loss: 0.0248\n",
      "Batch [400/1200], Time 0.03175830841064453\n",
      "Epoch [1/3], Step [500/1200] Loss: 0.0463\n",
      "Batch [500/1200], Time 0.0357973575592041\n",
      "Epoch [1/3], Step [600/1200] Loss: 0.0752\n",
      "Batch [600/1200], Time 0.036116600036621094\n",
      "Epoch [1/3], Step [700/1200] Loss: 0.0326\n",
      "Batch [700/1200], Time 0.031226396560668945\n",
      "Epoch [1/3], Step [800/1200] Loss: 0.0412\n",
      "Batch [800/1200], Time 0.031219005584716797\n",
      "Epoch [1/3], Step [900/1200] Loss: 0.0213\n",
      "Batch [900/1200], Time 0.040250301361083984\n",
      "Epoch [1/3], Step [1000/1200] Loss: 0.0210\n",
      "Batch [1000/1200], Time 0.03216266632080078\n",
      "Epoch [1/3], Step [1100/1200] Loss: 0.0844\n",
      "Batch [1100/1200], Time 0.03303670883178711\n",
      "Epoch [1/3], Step [1200/1200] Loss: 0.1802\n",
      "Epoch [1/3], Time 218.6721692085266\n",
      "Batch [0/1200], Time 0.03235197067260742\n",
      "Epoch [2/3], Step [100/1200] Loss: 0.0196\n",
      "Batch [100/1200], Time 0.03110671043395996\n",
      "Epoch [2/3], Step [200/1200] Loss: 0.0921\n",
      "Batch [200/1200], Time 0.05002331733703613\n",
      "Epoch [2/3], Step [300/1200] Loss: 0.0310\n",
      "Batch [300/1200], Time 0.03582286834716797\n",
      "Epoch [2/3], Step [400/1200] Loss: 0.0014\n",
      "Batch [400/1200], Time 0.033080101013183594\n",
      "Epoch [2/3], Step [500/1200] Loss: 0.0030\n",
      "Batch [500/1200], Time 0.030348539352416992\n",
      "Epoch [2/3], Step [600/1200] Loss: 0.0224\n",
      "Batch [600/1200], Time 0.03150033950805664\n",
      "Epoch [2/3], Step [700/1200] Loss: 0.0008\n",
      "Batch [700/1200], Time 0.04749941825866699\n",
      "Epoch [2/3], Step [800/1200] Loss: 0.0015\n",
      "Batch [800/1200], Time 0.03545641899108887\n",
      "Epoch [2/3], Step [900/1200] Loss: 0.0267\n",
      "Batch [900/1200], Time 0.03436565399169922\n",
      "Epoch [2/3], Step [1000/1200] Loss: 0.0080\n",
      "Batch [1000/1200], Time 0.033971548080444336\n",
      "Epoch [2/3], Step [1100/1200] Loss: 0.0613\n",
      "Batch [1100/1200], Time 0.04846811294555664\n",
      "Epoch [2/3], Step [1200/1200] Loss: 0.0248\n",
      "Epoch [2/3], Time 211.6463885307312\n",
      "Batch [0/1200], Time 0.033461809158325195\n",
      "Epoch [3/3], Step [100/1200] Loss: 0.0524\n",
      "Batch [100/1200], Time 0.0361635684967041\n",
      "Epoch [3/3], Step [200/1200] Loss: 0.0076\n",
      "Batch [200/1200], Time 0.03684091567993164\n",
      "Epoch [3/3], Step [300/1200] Loss: 0.0738\n",
      "Batch [300/1200], Time 0.02871870994567871\n",
      "Epoch [3/3], Step [400/1200] Loss: 0.0046\n",
      "Batch [400/1200], Time 0.032752275466918945\n",
      "Epoch [3/3], Step [500/1200] Loss: 0.0804\n",
      "Batch [500/1200], Time 0.033010244369506836\n",
      "Epoch [3/3], Step [600/1200] Loss: 0.0035\n",
      "Batch [600/1200], Time 0.037023067474365234\n",
      "Epoch [3/3], Step [700/1200] Loss: 0.0117\n",
      "Batch [700/1200], Time 0.03480243682861328\n",
      "Epoch [3/3], Step [800/1200] Loss: 0.0039\n",
      "Batch [800/1200], Time 0.032639265060424805\n",
      "Epoch [3/3], Step [900/1200] Loss: 0.0033\n",
      "Batch [900/1200], Time 0.03755903244018555\n",
      "Epoch [3/3], Step [1000/1200] Loss: 0.1440\n",
      "Batch [1000/1200], Time 0.03438282012939453\n",
      "Epoch [3/3], Step [1100/1200] Loss: 0.0203\n",
      "Batch [1100/1200], Time 0.03440523147583008\n",
      "Epoch [3/3], Step [1200/1200] Loss: 0.0056\n",
      "Epoch [3/3], Time 211.25705099105835\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    sta_time = time.time()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.expand(-1, 3, -1, -1)\n",
    "        sta_b_time = time.time()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_b_time = time.time()\n",
    "        if (i) % 100 == 0:\n",
    "            log = \"Batch [{}/{}], Time {}\".format(i, len(train_loader), end_b_time-sta_b_time)\n",
    "            print(log)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            log = \"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, total_step, loss.item())\n",
    "            print (log)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Epoch [{}/{}], Time {}\"\n",
    "                   .format(epoch+1, num_epochs, end_time-sta_time))\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ResNet50 + Cifar 10 + Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 ResNet50模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "class ImagenetTransferLearning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # init a pretrained resnet\n",
    "        backbone = models.resnet50(pretrained=False)\n",
    "        backbone.load_state_dict(torch.load('./checkpoints/resnet50-19c8e357.pth'))\n",
    "        num_filters = backbone.fc.in_features\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        self.feature_extractor = nn.Sequential(*layers)\n",
    "\n",
    "        # use the pretrained model to classify cifar-10 (10 image classes)\n",
    "        num_target_classes = 10\n",
    "        self.classifier = nn.Linear(num_filters, num_target_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.feature_extractor.eval()\n",
    "        with torch.no_grad():\n",
    "            representations = self.feature_extractor(x).flatten(1)\n",
    "        x = self.classifier(representations)\n",
    "        return x\n",
    "model = ImagenetTransferLearning()\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 准备数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Image preprocessing modules\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                             train=True, \n",
    "                                             transform=data_transforms['train'],\n",
    "                                             download=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data/',\n",
    "                                            train=False, \n",
    "                                            transform=data_transforms['val'])\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [0/500], Time 0.036528825759887695\n",
      "Batch [1/500], Time 0.023430585861206055\n",
      "Batch [2/500], Time 0.024915695190429688\n",
      "Batch [3/500], Time 0.0715031623840332\n",
      "Batch [4/500], Time 0.05972647666931152\n",
      "Batch [5/500], Time 0.039322853088378906\n",
      "Batch [6/500], Time 0.07664036750793457\n",
      "Batch [7/500], Time 0.026085615158081055\n",
      "Batch [8/500], Time 0.024243831634521484\n",
      "Batch [9/500], Time 0.05226445198059082\n",
      "Batch [10/500], Time 0.0223541259765625\n",
      "Batch [11/500], Time 0.07236552238464355\n",
      "Batch [12/500], Time 0.057222604751586914\n",
      "Batch [13/500], Time 0.046029090881347656\n",
      "Batch [14/500], Time 0.043637990951538086\n",
      "Batch [15/500], Time 0.04574465751647949\n",
      "Batch [16/500], Time 0.022602081298828125\n",
      "Batch [17/500], Time 0.022710084915161133\n",
      "Batch [18/500], Time 0.05513763427734375\n",
      "Batch [19/500], Time 0.04532766342163086\n",
      "Batch [20/500], Time 0.023118019104003906\n",
      "Batch [21/500], Time 0.07686424255371094\n",
      "Batch [22/500], Time 0.04053497314453125\n",
      "Batch [23/500], Time 0.02333855628967285\n",
      "Batch [24/500], Time 0.021627426147460938\n",
      "Batch [25/500], Time 0.022423982620239258\n",
      "Batch [26/500], Time 0.02168130874633789\n",
      "Batch [27/500], Time 0.0232999324798584\n",
      "Batch [28/500], Time 0.027579307556152344\n",
      "Batch [29/500], Time 0.051865577697753906\n",
      "Batch [30/500], Time 0.022615671157836914\n",
      "Batch [31/500], Time 0.02379775047302246\n",
      "Batch [32/500], Time 0.02790045738220215\n",
      "Batch [33/500], Time 0.04584360122680664\n",
      "Batch [34/500], Time 0.05847978591918945\n",
      "Batch [35/500], Time 0.023671388626098633\n",
      "Batch [36/500], Time 0.02297687530517578\n",
      "Batch [37/500], Time 0.07089877128601074\n",
      "Batch [38/500], Time 0.051431894302368164\n",
      "Batch [39/500], Time 0.021856307983398438\n",
      "Batch [40/500], Time 0.02432394027709961\n",
      "Batch [41/500], Time 0.04392433166503906\n",
      "Batch [42/500], Time 0.05864357948303223\n",
      "Batch [43/500], Time 0.04352259635925293\n",
      "Batch [44/500], Time 0.022542476654052734\n",
      "Batch [45/500], Time 0.05574440956115723\n",
      "Batch [46/500], Time 0.05441689491271973\n",
      "Batch [47/500], Time 0.02183222770690918\n",
      "Batch [48/500], Time 0.02748250961303711\n",
      "Batch [49/500], Time 0.023856401443481445\n",
      "Batch [50/500], Time 0.05154728889465332\n",
      "Batch [51/500], Time 0.04905271530151367\n",
      "Batch [52/500], Time 0.022360801696777344\n",
      "Batch [53/500], Time 0.052382707595825195\n",
      "Batch [54/500], Time 0.023095369338989258\n",
      "Batch [55/500], Time 0.04692220687866211\n",
      "Batch [56/500], Time 0.02179265022277832\n",
      "Batch [57/500], Time 0.0724802017211914\n",
      "Batch [58/500], Time 0.04462790489196777\n",
      "Batch [59/500], Time 0.03536629676818848\n",
      "Batch [60/500], Time 0.022635698318481445\n",
      "Batch [61/500], Time 0.03068828582763672\n",
      "Batch [62/500], Time 0.037071943283081055\n",
      "Batch [63/500], Time 0.07737278938293457\n",
      "Batch [64/500], Time 0.0435488224029541\n",
      "Batch [65/500], Time 0.043813228607177734\n",
      "Batch [66/500], Time 0.04903078079223633\n",
      "Batch [67/500], Time 0.05772256851196289\n",
      "Batch [68/500], Time 0.021422863006591797\n",
      "Batch [69/500], Time 0.032186269760131836\n",
      "Batch [70/500], Time 0.027103185653686523\n",
      "Batch [71/500], Time 0.07523488998413086\n",
      "Batch [72/500], Time 0.0670318603515625\n",
      "Batch [73/500], Time 0.04193115234375\n",
      "Batch [74/500], Time 0.025917768478393555\n",
      "Batch [75/500], Time 0.024260520935058594\n",
      "Batch [76/500], Time 0.02144789695739746\n",
      "Batch [77/500], Time 0.02587890625\n",
      "Batch [78/500], Time 0.022725343704223633\n",
      "Batch [79/500], Time 0.021907567977905273\n",
      "Batch [80/500], Time 0.021435260772705078\n",
      "Batch [81/500], Time 0.021564245223999023\n",
      "Batch [82/500], Time 0.029592037200927734\n",
      "Batch [83/500], Time 0.03208732604980469\n",
      "Batch [84/500], Time 0.021506786346435547\n",
      "Batch [85/500], Time 0.022852420806884766\n",
      "Batch [86/500], Time 0.056101083755493164\n",
      "Batch [87/500], Time 0.023459196090698242\n",
      "Batch [88/500], Time 0.02213764190673828\n",
      "Batch [89/500], Time 0.021949052810668945\n",
      "Batch [90/500], Time 0.035264015197753906\n",
      "Batch [91/500], Time 0.02752971649169922\n",
      "Batch [92/500], Time 0.03985857963562012\n",
      "Batch [93/500], Time 0.02262735366821289\n",
      "Batch [94/500], Time 0.02430582046508789\n",
      "Batch [95/500], Time 0.023605823516845703\n",
      "Batch [96/500], Time 0.03349661827087402\n",
      "Batch [97/500], Time 0.033577919006347656\n",
      "Batch [98/500], Time 0.026848554611206055\n",
      "Batch [99/500], Time 0.07532787322998047\n",
      "Epoch [1/1], Step [100/500] Loss: 0.8786\n",
      "Batch [100/500], Time 0.02692270278930664\n",
      "Batch [101/500], Time 0.06445050239562988\n",
      "Batch [102/500], Time 0.030120372772216797\n",
      "Batch [103/500], Time 0.02267169952392578\n",
      "Batch [104/500], Time 0.04694247245788574\n",
      "Batch [105/500], Time 0.021500825881958008\n",
      "Batch [106/500], Time 0.025412797927856445\n",
      "Batch [107/500], Time 0.048346757888793945\n",
      "Batch [108/500], Time 0.022093534469604492\n",
      "Batch [109/500], Time 0.021969079971313477\n",
      "Batch [110/500], Time 0.05443930625915527\n",
      "Batch [111/500], Time 0.04934287071228027\n",
      "Batch [112/500], Time 0.052783966064453125\n",
      "Batch [113/500], Time 0.030573129653930664\n",
      "Batch [114/500], Time 0.037123918533325195\n",
      "Batch [115/500], Time 0.022017240524291992\n",
      "Batch [116/500], Time 0.057137489318847656\n",
      "Batch [117/500], Time 0.03374814987182617\n",
      "Batch [118/500], Time 0.09433245658874512\n",
      "Batch [119/500], Time 0.08476734161376953\n",
      "Batch [120/500], Time 0.05851292610168457\n",
      "Batch [121/500], Time 0.024204730987548828\n",
      "Batch [122/500], Time 0.02237677574157715\n",
      "Batch [123/500], Time 0.022184371948242188\n",
      "Batch [124/500], Time 0.026177644729614258\n",
      "Batch [125/500], Time 0.021994829177856445\n",
      "Batch [126/500], Time 0.054086923599243164\n",
      "Batch [127/500], Time 0.02206563949584961\n",
      "Batch [128/500], Time 0.03421163558959961\n",
      "Batch [129/500], Time 0.021728992462158203\n",
      "Batch [130/500], Time 0.025570154190063477\n",
      "Batch [131/500], Time 0.060973167419433594\n",
      "Batch [132/500], Time 0.02207016944885254\n",
      "Batch [133/500], Time 0.07692456245422363\n",
      "Batch [134/500], Time 0.024465560913085938\n",
      "Batch [135/500], Time 0.022298812866210938\n",
      "Batch [136/500], Time 0.022097349166870117\n",
      "Batch [137/500], Time 0.0524439811706543\n",
      "Batch [138/500], Time 0.027081966400146484\n",
      "Batch [139/500], Time 0.049425363540649414\n",
      "Batch [140/500], Time 0.02341461181640625\n",
      "Batch [141/500], Time 0.062087297439575195\n",
      "Batch [142/500], Time 0.023517847061157227\n",
      "Batch [143/500], Time 0.022154808044433594\n",
      "Batch [144/500], Time 0.02370452880859375\n",
      "Batch [145/500], Time 0.0241241455078125\n",
      "Batch [146/500], Time 0.03180122375488281\n",
      "Batch [147/500], Time 0.02365708351135254\n",
      "Batch [148/500], Time 0.03010702133178711\n",
      "Batch [149/500], Time 0.023456573486328125\n",
      "Batch [150/500], Time 0.02255988121032715\n",
      "Batch [151/500], Time 0.07119226455688477\n",
      "Batch [152/500], Time 0.041496992111206055\n",
      "Batch [153/500], Time 0.021805524826049805\n",
      "Batch [154/500], Time 0.028310537338256836\n",
      "Batch [155/500], Time 0.051070451736450195\n",
      "Batch [156/500], Time 0.02200794219970703\n",
      "Batch [157/500], Time 0.060578346252441406\n",
      "Batch [158/500], Time 0.024866580963134766\n",
      "Batch [159/500], Time 0.02239370346069336\n",
      "Batch [160/500], Time 0.03345799446105957\n",
      "Batch [161/500], Time 0.02212214469909668\n",
      "Batch [162/500], Time 0.03147530555725098\n",
      "Batch [163/500], Time 0.024099111557006836\n",
      "Batch [164/500], Time 0.07782459259033203\n",
      "Batch [165/500], Time 0.022192716598510742\n",
      "Batch [166/500], Time 0.024897336959838867\n",
      "Batch [167/500], Time 0.04747414588928223\n",
      "Batch [168/500], Time 0.023104429244995117\n",
      "Batch [169/500], Time 0.058739662170410156\n",
      "Batch [170/500], Time 0.048159122467041016\n",
      "Batch [171/500], Time 0.06063723564147949\n",
      "Batch [172/500], Time 0.022165775299072266\n",
      "Batch [173/500], Time 0.060986995697021484\n",
      "Batch [174/500], Time 0.022298574447631836\n",
      "Batch [175/500], Time 0.049592018127441406\n",
      "Batch [176/500], Time 0.049988746643066406\n",
      "Batch [177/500], Time 0.022380352020263672\n",
      "Batch [178/500], Time 0.03900337219238281\n",
      "Batch [179/500], Time 0.02163386344909668\n",
      "Batch [180/500], Time 0.06313085556030273\n",
      "Batch [181/500], Time 0.02342987060546875\n",
      "Batch [182/500], Time 0.022608280181884766\n",
      "Batch [183/500], Time 0.04312419891357422\n",
      "Batch [184/500], Time 0.02924323081970215\n",
      "Batch [185/500], Time 0.059563636779785156\n",
      "Batch [186/500], Time 0.022320032119750977\n",
      "Batch [187/500], Time 0.022670984268188477\n",
      "Batch [188/500], Time 0.02160477638244629\n",
      "Batch [189/500], Time 0.022043466567993164\n",
      "Batch [190/500], Time 0.02182173728942871\n",
      "Batch [191/500], Time 0.02187061309814453\n",
      "Batch [192/500], Time 0.04267525672912598\n",
      "Batch [193/500], Time 0.05357718467712402\n",
      "Batch [194/500], Time 0.05793333053588867\n",
      "Batch [195/500], Time 0.07042598724365234\n",
      "Batch [196/500], Time 0.0307772159576416\n",
      "Batch [197/500], Time 0.030459165573120117\n",
      "Batch [198/500], Time 0.02322554588317871\n",
      "Batch [199/500], Time 0.022080421447753906\n",
      "Epoch [1/1], Step [200/500] Loss: 0.6144\n",
      "Batch [200/500], Time 0.023839235305786133\n",
      "Batch [201/500], Time 0.03728032112121582\n",
      "Batch [202/500], Time 0.023095130920410156\n",
      "Batch [203/500], Time 0.07220888137817383\n",
      "Batch [204/500], Time 0.034184932708740234\n",
      "Batch [205/500], Time 0.05028891563415527\n",
      "Batch [206/500], Time 0.031574249267578125\n",
      "Batch [207/500], Time 0.026716947555541992\n",
      "Batch [208/500], Time 0.07328104972839355\n",
      "Batch [209/500], Time 0.03409743309020996\n",
      "Batch [210/500], Time 0.027230501174926758\n",
      "Batch [211/500], Time 0.023540019989013672\n",
      "Batch [212/500], Time 0.021957874298095703\n",
      "Batch [213/500], Time 0.05211043357849121\n",
      "Batch [214/500], Time 0.02136707305908203\n",
      "Batch [215/500], Time 0.027399301528930664\n",
      "Batch [216/500], Time 0.02242279052734375\n",
      "Batch [217/500], Time 0.05452322959899902\n",
      "Batch [218/500], Time 0.022216796875\n",
      "Batch [219/500], Time 0.05246853828430176\n",
      "Batch [220/500], Time 0.052243947982788086\n",
      "Batch [221/500], Time 0.0227200984954834\n",
      "Batch [222/500], Time 0.021631956100463867\n",
      "Batch [223/500], Time 0.02891087532043457\n",
      "Batch [224/500], Time 0.04530835151672363\n",
      "Batch [225/500], Time 0.06003165245056152\n",
      "Batch [226/500], Time 0.06659412384033203\n",
      "Batch [227/500], Time 0.07534193992614746\n",
      "Batch [228/500], Time 0.02353835105895996\n",
      "Batch [229/500], Time 0.030592679977416992\n",
      "Batch [230/500], Time 0.03289651870727539\n",
      "Batch [231/500], Time 0.03717398643493652\n",
      "Batch [232/500], Time 0.022331953048706055\n",
      "Batch [233/500], Time 0.0226132869720459\n",
      "Batch [234/500], Time 0.022082090377807617\n",
      "Batch [235/500], Time 0.05235719680786133\n",
      "Batch [236/500], Time 0.08330082893371582\n",
      "Batch [237/500], Time 0.09230470657348633\n",
      "Batch [238/500], Time 0.03080010414123535\n",
      "Batch [239/500], Time 0.050628662109375\n",
      "Batch [240/500], Time 0.025068283081054688\n",
      "Batch [241/500], Time 0.02252483367919922\n",
      "Batch [242/500], Time 0.022266387939453125\n",
      "Batch [243/500], Time 0.06201887130737305\n",
      "Batch [244/500], Time 0.05296897888183594\n",
      "Batch [245/500], Time 0.07294797897338867\n",
      "Batch [246/500], Time 0.06860828399658203\n",
      "Batch [247/500], Time 0.030138015747070312\n",
      "Batch [248/500], Time 0.05119943618774414\n",
      "Batch [249/500], Time 0.026884794235229492\n",
      "Batch [250/500], Time 0.022743701934814453\n",
      "Batch [251/500], Time 0.025737524032592773\n",
      "Batch [252/500], Time 0.023886680603027344\n",
      "Batch [253/500], Time 0.023960590362548828\n",
      "Batch [254/500], Time 0.027415752410888672\n",
      "Batch [255/500], Time 0.024477005004882812\n",
      "Batch [256/500], Time 0.024381637573242188\n",
      "Batch [257/500], Time 0.057799339294433594\n",
      "Batch [258/500], Time 0.049027204513549805\n",
      "Batch [259/500], Time 0.06192660331726074\n",
      "Batch [260/500], Time 0.0650029182434082\n",
      "Batch [261/500], Time 0.06303668022155762\n",
      "Batch [262/500], Time 0.02406454086303711\n",
      "Batch [263/500], Time 0.021712303161621094\n",
      "Batch [264/500], Time 0.021805763244628906\n",
      "Batch [265/500], Time 0.02518463134765625\n",
      "Batch [266/500], Time 0.02213263511657715\n",
      "Batch [267/500], Time 0.028890132904052734\n",
      "Batch [268/500], Time 0.03161191940307617\n",
      "Batch [269/500], Time 0.053560733795166016\n",
      "Batch [270/500], Time 0.061409711837768555\n",
      "Batch [271/500], Time 0.05908942222595215\n",
      "Batch [272/500], Time 0.05872082710266113\n",
      "Batch [273/500], Time 0.03366494178771973\n",
      "Batch [274/500], Time 0.050313472747802734\n",
      "Batch [275/500], Time 0.03740811347961426\n",
      "Batch [276/500], Time 0.0322270393371582\n",
      "Batch [277/500], Time 0.04913759231567383\n",
      "Batch [278/500], Time 0.02934432029724121\n",
      "Batch [279/500], Time 0.02204585075378418\n",
      "Batch [280/500], Time 0.05651068687438965\n",
      "Batch [281/500], Time 0.03237032890319824\n",
      "Batch [282/500], Time 0.05671501159667969\n",
      "Batch [283/500], Time 0.02845931053161621\n",
      "Batch [284/500], Time 0.03842926025390625\n",
      "Batch [285/500], Time 0.04117465019226074\n",
      "Batch [286/500], Time 0.021425724029541016\n",
      "Batch [287/500], Time 0.03276634216308594\n",
      "Batch [288/500], Time 0.060352325439453125\n",
      "Batch [289/500], Time 0.0787973403930664\n",
      "Batch [290/500], Time 0.02484893798828125\n",
      "Batch [291/500], Time 0.021767377853393555\n",
      "Batch [292/500], Time 0.03378629684448242\n",
      "Batch [293/500], Time 0.027098417282104492\n",
      "Batch [294/500], Time 0.022411584854125977\n",
      "Batch [295/500], Time 0.055599212646484375\n",
      "Batch [296/500], Time 0.07230544090270996\n",
      "Batch [297/500], Time 0.02411627769470215\n",
      "Batch [298/500], Time 0.06183314323425293\n",
      "Batch [299/500], Time 0.021749496459960938\n",
      "Epoch [1/1], Step [300/500] Loss: 0.6787\n",
      "Batch [300/500], Time 0.02278876304626465\n",
      "Batch [301/500], Time 0.05565524101257324\n",
      "Batch [302/500], Time 0.028669118881225586\n",
      "Batch [303/500], Time 0.0216367244720459\n",
      "Batch [304/500], Time 0.021939754486083984\n",
      "Batch [305/500], Time 0.056879281997680664\n",
      "Batch [306/500], Time 0.02202463150024414\n",
      "Batch [307/500], Time 0.053524017333984375\n",
      "Batch [308/500], Time 0.028345108032226562\n",
      "Batch [309/500], Time 0.06485772132873535\n",
      "Batch [310/500], Time 0.05110597610473633\n",
      "Batch [311/500], Time 0.05787324905395508\n",
      "Batch [312/500], Time 0.026613950729370117\n",
      "Batch [313/500], Time 0.05646347999572754\n",
      "Batch [314/500], Time 0.05956125259399414\n",
      "Batch [315/500], Time 0.03325247764587402\n",
      "Batch [316/500], Time 0.021903276443481445\n",
      "Batch [317/500], Time 0.05717110633850098\n",
      "Batch [318/500], Time 0.06102275848388672\n",
      "Batch [319/500], Time 0.022406339645385742\n",
      "Batch [320/500], Time 0.023581504821777344\n",
      "Batch [321/500], Time 0.02309417724609375\n",
      "Batch [322/500], Time 0.026394367218017578\n",
      "Batch [323/500], Time 0.021715641021728516\n",
      "Batch [324/500], Time 0.05373048782348633\n",
      "Batch [325/500], Time 0.06049394607543945\n",
      "Batch [326/500], Time 0.06588482856750488\n",
      "Batch [327/500], Time 0.06091666221618652\n",
      "Batch [328/500], Time 0.0217435359954834\n",
      "Batch [329/500], Time 0.024342060089111328\n",
      "Batch [330/500], Time 0.03957986831665039\n",
      "Batch [331/500], Time 0.05273294448852539\n",
      "Batch [332/500], Time 0.02169179916381836\n",
      "Batch [333/500], Time 0.022127389907836914\n",
      "Batch [334/500], Time 0.021793127059936523\n",
      "Batch [335/500], Time 0.024230003356933594\n",
      "Batch [336/500], Time 0.03202104568481445\n",
      "Batch [337/500], Time 0.06639862060546875\n",
      "Batch [338/500], Time 0.027027130126953125\n",
      "Batch [339/500], Time 0.052706241607666016\n",
      "Batch [340/500], Time 0.0617067813873291\n",
      "Batch [341/500], Time 0.05735063552856445\n",
      "Batch [342/500], Time 0.03565573692321777\n",
      "Batch [343/500], Time 0.06332159042358398\n",
      "Batch [344/500], Time 0.02157902717590332\n",
      "Batch [345/500], Time 0.021989822387695312\n",
      "Batch [346/500], Time 0.022594451904296875\n",
      "Batch [347/500], Time 0.04052591323852539\n",
      "Batch [348/500], Time 0.03210735321044922\n",
      "Batch [349/500], Time 0.07131314277648926\n",
      "Batch [350/500], Time 0.032315731048583984\n",
      "Batch [351/500], Time 0.0232851505279541\n",
      "Batch [352/500], Time 0.02329397201538086\n",
      "Batch [353/500], Time 0.02606940269470215\n",
      "Batch [354/500], Time 0.05422472953796387\n",
      "Batch [355/500], Time 0.06296730041503906\n",
      "Batch [356/500], Time 0.022247791290283203\n",
      "Batch [357/500], Time 0.03640484809875488\n",
      "Batch [358/500], Time 0.05364060401916504\n",
      "Batch [359/500], Time 0.0436863899230957\n",
      "Batch [360/500], Time 0.032117366790771484\n",
      "Batch [361/500], Time 0.02295374870300293\n",
      "Batch [362/500], Time 0.0696723461151123\n",
      "Batch [363/500], Time 0.022304058074951172\n",
      "Batch [364/500], Time 0.05707716941833496\n",
      "Batch [365/500], Time 0.028140544891357422\n",
      "Batch [366/500], Time 0.05917549133300781\n",
      "Batch [367/500], Time 0.028828144073486328\n",
      "Batch [368/500], Time 0.03045654296875\n",
      "Batch [369/500], Time 0.047673702239990234\n",
      "Batch [370/500], Time 0.0658566951751709\n",
      "Batch [371/500], Time 0.0587613582611084\n",
      "Batch [372/500], Time 0.022119522094726562\n",
      "Batch [373/500], Time 0.02231121063232422\n",
      "Batch [374/500], Time 0.056246280670166016\n",
      "Batch [375/500], Time 0.024219036102294922\n",
      "Batch [376/500], Time 0.05371737480163574\n",
      "Batch [377/500], Time 0.05309176445007324\n",
      "Batch [378/500], Time 0.05168485641479492\n",
      "Batch [379/500], Time 0.06254148483276367\n",
      "Batch [380/500], Time 0.02252507209777832\n",
      "Batch [381/500], Time 0.021903514862060547\n",
      "Batch [382/500], Time 0.04504871368408203\n",
      "Batch [383/500], Time 0.03141927719116211\n",
      "Batch [384/500], Time 0.059783220291137695\n",
      "Batch [385/500], Time 0.05740618705749512\n",
      "Batch [386/500], Time 0.021603107452392578\n",
      "Batch [387/500], Time 0.026311874389648438\n",
      "Batch [388/500], Time 0.022060155868530273\n",
      "Batch [389/500], Time 0.04630398750305176\n",
      "Batch [390/500], Time 0.021820545196533203\n",
      "Batch [391/500], Time 0.0550081729888916\n",
      "Batch [392/500], Time 0.051703691482543945\n",
      "Batch [393/500], Time 0.03867697715759277\n",
      "Batch [394/500], Time 0.03367924690246582\n",
      "Batch [395/500], Time 0.046456098556518555\n",
      "Batch [396/500], Time 0.030469417572021484\n",
      "Batch [397/500], Time 0.029378175735473633\n",
      "Batch [398/500], Time 0.056110382080078125\n",
      "Batch [399/500], Time 0.029081344604492188\n",
      "Epoch [1/1], Step [400/500] Loss: 0.8687\n",
      "Batch [400/500], Time 0.02725386619567871\n",
      "Batch [401/500], Time 0.02513861656188965\n",
      "Batch [402/500], Time 0.021849393844604492\n",
      "Batch [403/500], Time 0.03857684135437012\n",
      "Batch [404/500], Time 0.07442736625671387\n",
      "Batch [405/500], Time 0.030797719955444336\n",
      "Batch [406/500], Time 0.022013187408447266\n",
      "Batch [407/500], Time 0.0525972843170166\n",
      "Batch [408/500], Time 0.022589683532714844\n",
      "Batch [409/500], Time 0.03325200080871582\n",
      "Batch [410/500], Time 0.044530391693115234\n",
      "Batch [411/500], Time 0.02114129066467285\n",
      "Batch [412/500], Time 0.061992645263671875\n",
      "Batch [413/500], Time 0.024603605270385742\n",
      "Batch [414/500], Time 0.05458521842956543\n",
      "Batch [415/500], Time 0.050692081451416016\n",
      "Batch [416/500], Time 0.022142648696899414\n",
      "Batch [417/500], Time 0.02114081382751465\n",
      "Batch [418/500], Time 0.021907329559326172\n",
      "Batch [419/500], Time 0.0582423210144043\n",
      "Batch [420/500], Time 0.06791138648986816\n",
      "Batch [421/500], Time 0.07601022720336914\n",
      "Batch [422/500], Time 0.03744196891784668\n",
      "Batch [423/500], Time 0.02158355712890625\n",
      "Batch [424/500], Time 0.02275872230529785\n",
      "Batch [425/500], Time 0.04005169868469238\n",
      "Batch [426/500], Time 0.021849870681762695\n",
      "Batch [427/500], Time 0.05912470817565918\n",
      "Batch [428/500], Time 0.04697775840759277\n",
      "Batch [429/500], Time 0.05654096603393555\n",
      "Batch [430/500], Time 0.02135777473449707\n",
      "Batch [431/500], Time 0.02271127700805664\n",
      "Batch [432/500], Time 0.0235595703125\n",
      "Batch [433/500], Time 0.09743952751159668\n",
      "Batch [434/500], Time 0.04905843734741211\n",
      "Batch [435/500], Time 0.06592321395874023\n",
      "Batch [436/500], Time 0.041098594665527344\n",
      "Batch [437/500], Time 0.02275991439819336\n",
      "Batch [438/500], Time 0.031171321868896484\n",
      "Batch [439/500], Time 0.07531547546386719\n",
      "Batch [440/500], Time 0.06316590309143066\n",
      "Batch [441/500], Time 0.03890347480773926\n",
      "Batch [442/500], Time 0.030764341354370117\n",
      "Batch [443/500], Time 0.023131370544433594\n",
      "Batch [444/500], Time 0.02377796173095703\n",
      "Batch [445/500], Time 0.05320477485656738\n",
      "Batch [446/500], Time 0.027948617935180664\n",
      "Batch [447/500], Time 0.030055761337280273\n",
      "Batch [448/500], Time 0.06696176528930664\n",
      "Batch [449/500], Time 0.040314435958862305\n",
      "Batch [450/500], Time 0.03918337821960449\n",
      "Batch [451/500], Time 0.06601476669311523\n",
      "Batch [452/500], Time 0.062354087829589844\n",
      "Batch [453/500], Time 0.02436232566833496\n",
      "Batch [454/500], Time 0.05987858772277832\n",
      "Batch [455/500], Time 0.02353048324584961\n",
      "Batch [456/500], Time 0.022507905960083008\n",
      "Batch [457/500], Time 0.05758094787597656\n",
      "Batch [458/500], Time 0.026197195053100586\n",
      "Batch [459/500], Time 0.029437541961669922\n",
      "Batch [460/500], Time 0.02868962287902832\n",
      "Batch [461/500], Time 0.022720813751220703\n",
      "Batch [462/500], Time 0.06358647346496582\n",
      "Batch [463/500], Time 0.07802867889404297\n",
      "Batch [464/500], Time 0.08581709861755371\n",
      "Batch [465/500], Time 0.04124951362609863\n",
      "Batch [466/500], Time 0.02184271812438965\n",
      "Batch [467/500], Time 0.04282641410827637\n",
      "Batch [468/500], Time 0.08826184272766113\n",
      "Batch [469/500], Time 0.022234201431274414\n",
      "Batch [470/500], Time 0.022644519805908203\n",
      "Batch [471/500], Time 0.06445646286010742\n",
      "Batch [472/500], Time 0.061922311782836914\n",
      "Batch [473/500], Time 0.07262372970581055\n",
      "Batch [474/500], Time 0.028809547424316406\n",
      "Batch [475/500], Time 0.058196306228637695\n",
      "Batch [476/500], Time 0.04776787757873535\n",
      "Batch [477/500], Time 0.05686593055725098\n",
      "Batch [478/500], Time 0.08262348175048828\n",
      "Batch [479/500], Time 0.09192490577697754\n",
      "Batch [480/500], Time 0.03164386749267578\n",
      "Batch [481/500], Time 0.028316259384155273\n",
      "Batch [482/500], Time 0.06772947311401367\n",
      "Batch [483/500], Time 0.02225184440612793\n",
      "Batch [484/500], Time 0.023635387420654297\n",
      "Batch [485/500], Time 0.02146148681640625\n",
      "Batch [486/500], Time 0.02201986312866211\n",
      "Batch [487/500], Time 0.06621336936950684\n",
      "Batch [488/500], Time 0.022054433822631836\n",
      "Batch [489/500], Time 0.02967357635498047\n",
      "Batch [490/500], Time 0.05302596092224121\n",
      "Batch [491/500], Time 0.06006813049316406\n",
      "Batch [492/500], Time 0.029544830322265625\n",
      "Batch [493/500], Time 0.021985292434692383\n",
      "Batch [494/500], Time 0.07221603393554688\n",
      "Batch [495/500], Time 0.029086828231811523\n",
      "Batch [496/500], Time 0.02168107032775879\n",
      "Batch [497/500], Time 0.021656274795532227\n",
      "Batch [498/500], Time 0.054297685623168945\n",
      "Batch [499/500], Time 0.022278547286987305\n",
      "Epoch [1/1], Step [500/500] Loss: 0.9889\n",
      "Epoch [1/1], Time 180.21306824684143\n"
     ]
    }
   ],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# For updating learning rate\n",
    "def update_lr(optimizer, lr):    \n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "curr_lr = learning_rate\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    sta_time = time.time()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        sta_b_time = time.time()\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_b_time = time.time()\n",
    "        log = \"Batch [{}/{}], Time {}\".format(i, len(train_loader), end_b_time-sta_b_time)\n",
    "        with open('./results/log_{}.txt'.format(gpu_flag), 'a') as f:\n",
    "            f.write(log + \"\\n\")\n",
    "        print(log)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            log = \"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch+1, num_epochs, i+1, total_step, loss.item())\n",
    "            print (log)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"Epoch [{}/{}], Time {}\"\n",
    "                   .format(epoch+1, num_epochs, end_time-sta_time))\n",
    "    # Decay learning rate\n",
    "    if (epoch+1) % 20 == 0:\n",
    "        curr_lr /= 3\n",
    "        update_lr(optimizer, curr_lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 测试模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 86.68 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.5 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), './checkpoints/resnet.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1.7.1",
   "language": "python",
   "name": "pytorch-1.7.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
